# pytest configuration for VeriFlowCC
# Comprehensive test settings with coverage requirements and performance optimizations

[tool:pytest]
# Test discovery patterns
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Directory configuration
testpaths = tests
norecursedirs = .git .tox dist build *.egg __pycache__ .venv venv .cache

# Minimum version requirement
minversion = 7.0

# Add current directory to Python path
pythonpath = .

# Strict mode settings
strict_markers = true
strict_config = true

# Output and reporting
addopts =
    # Verbose output with short test summary
    -v
    --tb=short
    --strict-markers

    # Coverage settings with 80% minimum threshold
    --cov=verifflowcc
    --cov-report=term-missing:skip-covered
    --cov-report=html:htmlcov
    --cov-report=xml
    --cov-report=json
    --cov-fail-under=80
    --cov-branch

    # Parallel execution for performance
    -n auto
    --dist=loadscope

    # Async support
    --asyncio-mode=auto

    # Show slowest tests
    --durations=10

    # Fail on warnings
    --strict
    -W error::DeprecationWarning
    -W error::PendingDeprecationWarning

    # Doctest modules
    --doctest-modules
    --doctest-continue-on-failure

    # Output formatting
    --color=yes
    --show-capture=no

    # JUnit XML for CI/CD integration
    --junitxml=test-results/junit.xml

    # JSON report for detailed analysis
    --json-report
    --json-report-file=test-results/report.json

    # Fail fast option (commented out by default)
    # --exitfirst
    # --maxfail=3

# Test markers for organization and selective execution
markers =
    # V-Model stage markers
    planning: Tests for planning stage functionality
    requirements: Tests for requirements analysis
    design: Tests for architecture and design
    coding: Tests for implementation
    testing: Tests for test execution
    integration: Tests for integration validation
    validation: Tests for acceptance validation
    verification: Test suite verification tests

    # Test type markers
    unit: Unit tests (fast, isolated)
    integration: Integration tests (slower, dependencies)
    e2e: End-to-end tests (slowest, full workflow)
    workflow: Workflow integration tests
    smoke: Smoke tests (critical path validation)
    regression: Regression tests

    # Performance markers
    slow: Tests that take >1s to run
    performance: Performance benchmark tests
    stress: Stress and load tests

    # Special markers
    gate: Gate validation tests
    agent: Agent-specific tests
    cli: CLI interface tests
    mcp: MCP integration tests

    # Environment markers
    requires_api_key: Tests requiring API keys
    requires_network: Tests requiring network access
    requires_docker: Tests requiring Docker

    # Priority markers
    critical: Critical functionality tests
    high: High priority tests
    medium: Medium priority tests
    low: Low priority tests

# Coverage configuration
[coverage:run]
source = verifflowcc
branch = True
parallel = True
omit =
    */tests/*
    */test_*.py
    */__pycache__/*
    */site-packages/*
    */venv/*
    */.venv/*
    */migrations/*
    */config/*
    */setup.py
    */conftest.py

[coverage:report]
# Coverage report settings
precision = 2
show_missing = True
skip_covered = False
skip_empty = True

# Exclude specific code patterns from coverage
exclude_lines =
    # Standard pragma
    pragma: no cover

    # Debugging code
    def __repr__
    def __str__

    # Defensive programming
    raise AssertionError
    raise NotImplementedError

    # Non-runnable code
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstract
    @abstractmethod

    # Error handling
    except ImportError
    except KeyError
    except Exception

    # Platform-specific code
    if sys.platform

    # Optional features
    except ImportError as e:

    # Logging
    logger.debug
    logger.trace

# Coverage thresholds per module
[coverage:report:verifflowcc.agents]
fail_under = 85

[coverage:report:verifflowcc.core]
fail_under = 90

[coverage:report:verifflowcc.schemas]
fail_under = 95

[coverage:report:verifflowcc.cli]
fail_under = 80

# HTML coverage report
[coverage:html]
directory = htmlcov
title = VeriFlowCC Test Coverage Report
show_contexts = True

# XML coverage report for CI/CD
[coverage:xml]
output = coverage.xml

# JSON coverage report for analysis
[coverage:json]
output = coverage.json
pretty_print = True
show_contexts = True

# Async testing configuration
[tool:pytest:asyncio]
# Default async test mode
asyncio_mode = auto

# Logging configuration for tests
[tool:pytest:logging]
# Capture log messages during tests
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] [%(name)s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# File logging for test runs
log_file = test-results/test.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] [%(name)s] %(filename)s:%(lineno)d - %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Timeout configuration
[tool:pytest:timeout]
# Global timeout for tests (in seconds)
timeout = 300
# Timeout method (thread or signal)
timeout_method = thread
# Timeout for async tests
timeout_func_only = false

# Flaky test handling
[tool:pytest:flaky]
max_runs = 3
min_passes = 1

# Benchmark configuration
[tool:pytest:benchmark]
# Store benchmark results
storage = file://test-results/benchmarks
# Compare against previous results
compare = last
# Fail if performance regresses by more than 10%
max_time = 0.1
min_rounds = 5
warmup = true
warmup_iterations = 1

# Hypothesis settings for property-based testing
[tool:pytest:hypothesis]
# Hypothesis profile for CI
profile = ci
max_examples = 100
deadline = 5000
print_blob = true

# Custom test directories and patterns
[tool:pytest:custom]
# Agent-specific test directories
agent_tests = tests/agents/
# Integration test directories
integration_tests = tests/integration/
# E2E test directories
e2e_tests = tests/e2e/

# Test data directories
[tool:pytest:testdata]
fixtures = tests/fixtures/
snapshots = tests/snapshots/
mocks = tests/mocks/

# Environment variables for testing
[tool:pytest:env]
TEST_ENV = testing
PYTHONDONTWRITEBYTECODE = 1
PYTEST_CURRENT_TEST = 1

# Parallel execution settings
[tool:pytest:xdist]
# Number of workers (auto = number of CPU cores)
numprocesses = auto
# Load balancing strategy
dist = loadscope
# Rerun failed tests on a single worker
reruns = 3
reruns_delay = 1

# Mutation testing configuration (for test quality)
[tool:pytest:mutmut]
paths_to_mutate = verifflowcc/
tests_dir = tests/
runner = pytest
dict_synonyms =
    Dict, OrderedDict, defaultdict
    Set, FrozenSet
    List, Tuple
