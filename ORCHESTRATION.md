# Tool Orchestration and Integration (MCP, Hooks, and Automation)

AgileVerifFlowCC’s orchestration logic ties together Claude-Code’s capabilities with external tool integration to create a seamless automated workflow. We leverage Anthropic’s Model Context Protocol (MCP) and Claude-Code hooks to enhance this orchestration:

- Model Context Protocol (MCP) Integration: MCP provides a standardized interface for Claude (or any LLM agent) to call external tools and services in a controlled manner ￼. We use MCP both implicitly (Claude-Code’s internal tools like the file system and terminal already adhere to an MCP-like interface) and explicitly for any additional integrations. For example, if the project requires interacting with external APIs or resources (imagine a requirement to verify something on a live server), we could define an MCP tool for that. A concrete use-case included in references is integrating with a GitHub MCP server to let Claude read repository content and manage PRs ￼. In our case, possible MCP usages are:

- A GitHub integration to push code or create PRs at the end of a sprint. With .mcp.json configured (pointing to the GitHub MCP server and auth token) ￼ ￼, Claude could use @github commands to create a pull request or update a README. This is optional but demonstrates the extendability of the tool for real-world dev workflows.

- A Documentation lookup tool: if the project has large external docs (say an API spec PDF), an MCP tool could allow Claude to query those as needed during design or testing.

- Using MCP as the backbone for subagent invocation. In a multi-agent architecture, one agent could call another via an MCP call (since each subagent can be exposed as a “service”). However, Claude-Code already can switch to subagents internally via prompt, so we might not need this. But conceptually, the Planner-Worker pattern we discussed could be implemented where the Planner (main prompt) calls subagents as if they were external tools – MCP would format those requests. This aligns with architecture Pattern 1 from our references, where the Planner uses MCP to hand off tasks ￼ ￼. We might not implement a full MCP server for our subagents in the initial version (since Claude can spawn them internally), but it’s a potential expansion for robustness.
  The benefit of MCP is consistent, debuggable interactions. Instead of custom hacky prompts for tool use, we rely on a protocol that defines inputs/outputs. This reduces errors and makes it easier to secure actions (MCP can enforce authentication and rate limits on external calls, for instance). Our CLI will encourage using existing MCP tool definitions for common needs (like the bash tool, the git tool, etc.) rather than reinventing them.

- Claude-Code Hooks for Automation: Hooks allow us to inject deterministic actions at certain points in Claude’s operation ￼ ￼. We plan to register a set of hooks to enforce workflow and quality rules automatically:

- PreToolUse Hook: We can register a pre-tool hook on file write or shell execution to enforce gating. For example, a PreToolUse for the edit tool could check if the file being edited is a test file vs. a code file and whether the corresponding requirement is defined. If Developer agent tries to write code for a story that has no acceptance criteria, the hook can intercept and prompt Claude with an error or reminder (effectively preventing that action until requirements phase is done). Similarly, before any shell test run, a hook could log the test command and ensure we have permission to run it (for security).

- PostToolUse Hook: After certain tools run, we can trigger automatic follow-ups. A great use: after the Developer agent writes code (file saved), run a lint or format command automatically (Claude can then see the diff and adjust if formatting changed anything). Or after running tests, automatically generate a coverage report. These things can be done by Claude if instructed, but hooks ensure they happen every time without relying on the AI’s “memory” to do so ￼. It turns best practices into guaranteed actions.

- Notification Hook: This could be used to notify the user (or a log) when the AI is awaiting input. For instance, after finishing its part, Claude might be waiting for user confirmation to proceed – we could tie into that to display a special message or send a desktop notification. Not critical, but a nice UX improvement (and mentioned by Anthropic docs as a use case) ￼.

- Stop Hook (on conversation stop): When Claude finishes responding for a phase, we can use this to auto-save outputs. For example, if Claude’s last answer was the content of a test file, the Stop hook can capture that and write it to the actual file system (though our orchestrator can do that too by parsing responses). Essentially, hooks can serve as a backup to ensure everything Claude outputs as a file edit indeed gets saved.

- Subagent Stop Hook: Anthropic mentions a hook event for when subagents complete tasks ￼. We can utilize this to chain between phases. For instance, when the Requirements subagent finishes, a Subagent Stop hook could automatically trigger the Design subagent next (if in auto-run mode). This is another way to implement the pipeline without waiting for user input – the hook can instruct the orchestrator or directly call the next phase. We have to be careful to avoid infinite loops, but a controlled sequence is feasible.

Using hooks, we encode the workflow logic into the tool’s infrastructure rather than relying solely on prompt discipline ￼ ￼. This provides rigidity where needed (particularly in hard-gating mode). For example, no matter what, if tests fail, a hook could prevent the /finish of a sprint from executing, or if a certain quality metric isn’t met, the hook could inject a request for Claude to improve documentation. These are deterministic checkpoints that don’t depend on the model’s sometimes unpredictable behavior.

- Multi-step Orchestration and Safety: The orchestrator, possibly in conjunction with hooks, implements a plan-then-act loop. The CLI might explicitly ask Claude for a plan (list of steps) at the start of a sprint, then execute step by step, using subagents/tools accordingly – similar to how Claude Code’s checklist mode works, but under our program’s control ￼ ￼. This yields transparency and the ability to catch errors early. For safety, we also incorporate rollback mechanisms as noted in the literature: e.g. make a git commit before a major change, so if something goes awry (tests introduce a regression that can’t be fixed quickly), we can revert to last good state ￼. The CLI could integrate with git locally, committing at the end of each phase or at checkpoints, which is a straightforward and effective safety net.

- External Orchestration (MCP/ADK): In future or advanced usage, AgileVerifFlowCC could be combined with agent orchestration frameworks (like Google’s Agent Development Kit, as in the blog example ￼ ￼) for even more complex flows (like involving multiple AI agents simultaneously). However, our design generally sticks to one Claude instance with subagents to reduce complexity. As an aside, since only one user is involved, we don’t handle multi-user synchronization issues – but the single orchestrator approach conveniently avoids race conditions by design.

In essence, by blending Claude’s intelligence with programmatic guardrails (MCP calls and hooks), we achieve a balance: the AI handles creative and complex tasks (writing code, formulating tests) while the CLI framework handles enforcement and repetition. This ensures the Agile V-model process isn’t left to chance or the model’s whims – it’s systematically reinforced by the environment ￼. The end result is a reliable pipeline where tasks happen in the right order, and any deviation is caught either by the AI self-correcting (due to prompts) or by the CLI’s hooks and checks.
