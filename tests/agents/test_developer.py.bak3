"""Tests for DeveloperAgent (Coding stage agent).

This module tests the DeveloperAgent functionality including feature implementation,
code generation, and implementation reporting.
"""

import json
from pathlib import Path
from typing import Any
from unittest.mock import patch

import pytest
from verifflowcc.agents.developer import DeveloperAgent
from verifflowcc.core.vmodel import VModelStage
from verifflowcc.schemas.agent_schemas import ImplementationInput, ImplementationOutput


class TestDeveloperAgentInitialization:
    """Test DeveloperAgent initialization and configuration."""

    def test_developer_agent_initialization(self, isolated_agilevv_dir):
        """Test DeveloperAgent initializes correctly."""
        agent = DeveloperAgent(name="developer", path_config=isolated_agilevv_dir)

        assert agent.name == "developer"
        assert agent.path_config == isolated_agilevv_dir
        assert agent.model == "claude-3-sonnet"  # default value
        assert agent.max_tokens == 4000  # default value

    def test_developer_agent_custom_configuration(self, isolated_agilevv_dir):
        """Test DeveloperAgent with custom configuration."""
        agent = DeveloperAgent(
            name="custom_developer",
            model="claude-3-opus",
            max_tokens=8000,
            path_config=isolated_agilevv_dir,
        )

        assert agent.name == "custom_developer"
        assert agent.model == "claude-3-opus"
        assert agent.max_tokens == 8000


class TestDeveloperAgentInputValidation:
    """Test DeveloperAgent input validation and processing."""

    def test_implementation_input_validation(self, isolated_agilevv_dir):
        """Test that DeveloperAgent validates ImplementationInput correctly."""
        # Instantiate agent to verify it can be created with valid config
        DeveloperAgent(path_config=isolated_agilevv_dir)

        # Valid input
        valid_input = ImplementationInput(
            story_id="US-001",
            stage=VModelStage.CODING,
            context={"design_complete": True},
            design_artifacts={
                "components": ["UserService", "AuthService"],
                "interfaces": ["IUserRepository"],
            },
            architecture_context={
                "existing_services": ["CoreService"],
                "database_schema": "users_table",
            },
        )

        # Should not raise any validation errors
        assert valid_input.story_id == "US-001"
        assert valid_input.stage == VModelStage.CODING
        assert valid_input.design_artifacts is not None

    def test_implementation_input_missing_design_artifacts(self):
        """Test that ImplementationInput requires design artifacts."""
        with pytest.raises(ValueError, match="design_artifacts cannot be empty"):
            ImplementationInput(
                story_id="US-001",
                stage=VModelStage.CODING,
                context={},
                design_artifacts={},  # Empty - should fail
                architecture_context={"context": "data"},
            )


class TestDeveloperAgentArtifactManagement:
    """Test DeveloperAgent artifact creation and management."""

    def test_save_implementation_artifacts(self, isolated_agilevv_dir):
        """Test saving implementation artifacts to correct subdirectory."""
        agent = DeveloperAgent(path_config=isolated_agilevv_dir)

        implementation_data = {
            "source_files": ["src/user_service.py", "src/auth_service.py"],
            "code_metrics": {"lines": 150, "complexity": 8},
            "features_implemented": ["login", "logout", "user_creation"],
        }

        # Save implementation artifact
        agent.save_artifact("implementation/US-001.json", implementation_data)

        # Verify file was created in correct location
        artifact_path = isolated_agilevv_dir.base_dir / "implementation" / "US-001.json"
        assert artifact_path.exists()

        # Verify content
        loaded_data = json.loads(artifact_path.read_text())
        assert loaded_data == implementation_data

    def test_create_source_files(self, isolated_agilevv_dir):
        """Test creating source files during implementation."""
        # Instantiate agent to verify it can be created with valid config
        DeveloperAgent(path_config=isolated_agilevv_dir)

        # Create test source file
        source_content = """
class UserService:
    def __init__(self):
        self.users = []

    def create_user(self, name, email):
        user = {"name": name, "email": email}
        self.users.append(user)
        return user
"""

        # This would be done by the agent during processing
        source_dir = isolated_agilevv_dir.base_dir.parent / "src"
        source_dir.mkdir(parents=True, exist_ok=True)
        source_file = source_dir / "user_service.py"
        source_file.write_text(source_content)

        # Verify file was created
        assert source_file.exists()
        assert "class UserService" in source_file.read_text()

    def test_load_implementation_artifacts(self, isolated_agilevv_dir):
        """Test loading existing implementation artifacts."""
        agent = DeveloperAgent(path_config=isolated_agilevv_dir)

        # Create test artifact
        impl_data = {"files": ["test.py"], "metrics": {"lines": 50}}
        artifact_path = isolated_agilevv_dir.base_dir / "implementation"
        artifact_path.mkdir(parents=True, exist_ok=True)
        (artifact_path / "US-002.json").write_text(json.dumps(impl_data))

        # Load artifact
        loaded_data = agent.load_artifact("implementation/US-002.json")
        assert loaded_data == impl_data


@pytest.mark.asyncio
class TestDeveloperAgentProcessing:
    """Test DeveloperAgent main processing functionality."""

    @patch("verifflowcc.agents.developer.DeveloperAgent._call_claude_api")
    async def test_process_implementation_generation(self, mock_claude_api, isolated_agilevv_dir):
        """Test the main process method for implementation generation."""
        # Setup mock response
        mock_response = {
            "source_files": [
                {"path": "src/user_service.py", "content": "class UserService:\n    pass"},
                {"path": "src/auth_service.py", "content": "class AuthService:\n    pass"},
            ],
            "code_metrics": {
                "total_lines": 150,
                "complexity_score": 5,
                "test_coverage": 85,
                "maintainability_index": 90,
            },
            "implementation_report": {
                "features_implemented": ["user_registration", "authentication"],
                "design_patterns_used": ["Repository", "Service"],
                "dependencies_added": ["bcrypt", "jwt"],
            },
        }
        mock_claude_api.return_value = mock_response

        agent = DeveloperAgent(path_config=isolated_agilevv_dir)

        # Create input
        impl_input = ImplementationInput(
            story_id="US-001",
            stage=VModelStage.CODING,
            context={"sprint": 1, "priority": "high"},
            design_artifacts={
                "components": ["UserService", "AuthService"],
                "interfaces": ["IUserRepository", "IAuthProvider"],
                "data_models": ["User", "Session"],
            },
            architecture_context={"existing_components": ["CoreService"], "database": "PostgreSQL"},
        )

        # Process
        result = await agent.process(impl_input.model_dump())

        # Validate result structure
        assert isinstance(result, dict)
        assert result["status"] == "success"
        assert result["next_stage_ready"] is True
        assert "artifacts" in result
        assert "source_files" in result
        assert "code_metrics" in result
        assert "implementation_report" in result

        # Validate that artifacts were saved
        impl_artifact_path = isolated_agilevv_dir.base_dir / "implementation" / "US-001.json"
        assert impl_artifact_path.exists()

    @patch("verifflowcc.agents.developer.DeveloperAgent._call_claude_api")
    async def test_process_with_api_failure(self, mock_claude_api, isolated_agilevv_dir):
        """Test process method handles API failures gracefully."""
        # Setup mock to raise an exception
        mock_claude_api.side_effect = Exception("API Error: Service unavailable")

        agent = DeveloperAgent(path_config=isolated_agilevv_dir)

        impl_input = ImplementationInput(
            story_id="US-002",
            stage=VModelStage.CODING,
            context={},
            design_artifacts={"components": ["TestService"]},
            architecture_context={"db": "SQLite"},
        )

        # Process should handle the error
        result = await agent.process(impl_input.model_dump())

        assert result["status"] == "error"
        assert result["next_stage_ready"] is False
        assert len(result["errors"]) > 0
        assert "API Error" in result["errors"][0]

    @patch("verifflowcc.agents.developer.DeveloperAgent._call_claude_api")
    async def test_process_partial_implementation(self, mock_claude_api, isolated_agilevv_dir):
        """Test process method handles partial implementation scenarios."""
        # Setup mock with incomplete response
        mock_response = {
            "source_files": [
                {"path": "src/user_service.py", "content": "# Incomplete implementation"}
            ],
            "code_metrics": {
                "total_lines": 20  # Low lines indicate partial implementation
            },
            # Missing implementation_report
        }
        mock_claude_api.return_value = mock_response

        agent = DeveloperAgent(path_config=isolated_agilevv_dir)

        impl_input = ImplementationInput(
            story_id="US-003",
            stage=VModelStage.CODING,
            context={},
            design_artifacts={"components": ["UserService"]},
            architecture_context={"framework": "FastAPI"},
        )

        result = await agent.process(impl_input.model_dump())

        # Should handle partial results
        assert result["status"] in ["partial", "success"]
        assert "artifacts" in result


class TestDeveloperAgentIntegration:
    """Integration tests for DeveloperAgent with V-Model workflow."""

    def test_integration_with_architect_output(self, isolated_agilevv_dir):
        """Test DeveloperAgent can process ArchitectAgent output."""
        # Instantiate agent to verify it can be created with valid config
        DeveloperAgent(path_config=isolated_agilevv_dir)

        # Simulate architect agent output
        design_output = {
            "design_specifications": {
                "components": ["UserService", "AuthService"],
                "interfaces": ["IUserRepository", "IAuthProvider"],
                "data_models": ["User", "Session", "Credentials"],
            },
            "interface_contracts": {
                "IUserRepository": {
                    "methods": ["findById", "save", "delete"],
                    "contracts": ["User findById(String id)", "boolean save(User user)"],
                }
            },
        }

        # Create ImplementationInput from design
        impl_input = ImplementationInput(
            story_id="US-001",
            stage=VModelStage.CODING,
            context={"source": "architect_agent"},
            design_artifacts=design_output,
            architecture_context={"framework": "Spring Boot"},
        )

        # Should validate successfully
        assert impl_input.story_id == "US-001"
        assert impl_input.design_artifacts == design_output

    def test_implementation_output_validation(self, isolated_agilevv_dir):
        """Test ImplementationOutput validation for next stage."""
        # Create valid implementation output
        impl_output_data = {
            "status": "success",
            "artifacts": {"implementation_report": "path/to/impl.json"},
            "source_files": ["src/user_service.py", "src/auth_service.py"],
            "code_metrics": {"total_lines": 200, "complexity_score": 6, "test_coverage": 90},
            "implementation_report": {
                "features_implemented": ["login", "registration"],
                "patterns_used": ["Repository", "Service"],
                "quality_score": 8.5,
            },
            "next_stage_ready": True,
        }

        # Should validate successfully
        impl_output = ImplementationOutput(**impl_output_data)
        assert impl_output.status == "success"
        assert impl_output.next_stage_ready is True
        assert len(impl_output.source_files) == 2

    def test_error_handling_with_invalid_source_files(self):
        """Test that ImplementationOutput validates source file types."""
        from pydantic_core import ValidationError

        with pytest.raises(ValidationError, match="Input should be a valid string"):
            ImplementationOutput(
                status="success",
                artifacts={},
                source_files=["valid_file.py", 123, "another_file.py"],  # Invalid type
                code_metrics={},
                implementation_report={},
            )


class TestDeveloperAgentCodeGeneration:
    """Test DeveloperAgent code generation capabilities."""

    def test_code_quality_validation(self, isolated_agilevv_dir):
        """Test that DeveloperAgent validates code quality."""
        agent = DeveloperAgent(path_config=isolated_agilevv_dir)

        # Test different code quality scenarios
        high_quality_metrics = {
            "total_lines": 150,
            "complexity_score": 4,
            "test_coverage": 95,
            "maintainability_index": 85,
        }

        low_quality_metrics = {
            "total_lines": 1000,
            "complexity_score": 15,
            "test_coverage": 30,
            "maintainability_index": 40,
        }

        # TODO: Implement _validate_code_quality method in DeveloperAgent
        # High quality should pass validation
        # assert agent._validate_code_quality(high_quality_metrics) is True

        # Low quality should fail validation
        # assert agent._validate_code_quality(low_quality_metrics) is False

    def test_source_file_creation(self, isolated_agilevv_dir):
        """Test creating source files with proper structure."""
        agent = DeveloperAgent(path_config=isolated_agilevv_dir)

        source_files_data = [
            {
                "path": "src/services/user_service.py",
                "content": """
from typing import List, Optional
from models.user import User

class UserService:
    def __init__(self, repository):
        self.repository = repository

    def create_user(self, name: str, email: str) -> User:
        user = User(name=name, email=email)
        return self.repository.save(user)

    def find_user(self, user_id: str) -> Optional[User]:
        return self.repository.find_by_id(user_id)
""",
            },
            {
                "path": "src/models/user.py",
                "content": """
from dataclasses import dataclass

@dataclass
class User:
    name: str
    email: str
    id: Optional[str] = None
""",
            },
        ]

        # TODO: Implement _create_source_files method in DeveloperAgent
        # This would be called during agent processing
        # created_files = agent._create_source_files(source_files_data)

        # assert len(created_files) == 2
        # assert "src/services/user_service.py" in created_files
        # assert "src/models/user.py" in created_files


@pytest.mark.asyncio
class TestDeveloperAgentErrorRecovery:
    """Test DeveloperAgent error handling and recovery mechanisms."""

    @patch("verifflowcc.agents.developer.DeveloperAgent._call_claude_api")
    async def test_retry_mechanism_for_compilation_errors(
        self, mock_claude_api, isolated_agilevv_dir
    ):
        """Test that agent retries when generated code has compilation errors."""
        # First call generates code with syntax errors, second call fixes them
        mock_claude_api.side_effect = [
            Exception("Compilation error: Invalid syntax"),
            {
                "source_files": [{"path": "src/service.py", "content": "class Service:\n    pass"}],
                "code_metrics": {
                    "total_lines": 10,
                    "complexity_score": 1,
                    "test_coverage": 90,
                    "maintainability_index": 85,
                },
                "implementation_report": {"features": ["basic_service"]},
            },
        ]

        agent = DeveloperAgent(path_config=isolated_agilevv_dir)

        impl_input = ImplementationInput(
            story_id="US-004",
            stage=VModelStage.CODING,
            context={},
            design_artifacts={"components": ["Service"]},
            architecture_context={"language": "Python"},
        )

        # Should succeed on retry
        result = await agent.process(impl_input.model_dump())
        assert result["status"] == "success"
        assert mock_claude_api.call_count == 2

    async def test_validation_error_handling(self, isolated_agilevv_dir):
        """Test handling of input validation errors."""
        agent = DeveloperAgent(path_config=isolated_agilevv_dir)

        # Invalid input - missing required fields
        invalid_input = {
            "story_id": "US-005",
            "stage": VModelStage.CODING,
            "context": {},
            # Missing design_artifacts and architecture_context
        }

        result = await agent.process(invalid_input)

        assert result["status"] == "error"
        assert result["next_stage_ready"] is False
        assert len(result["errors"]) > 0
        assert any("validation" in error.lower() for error in result["errors"])


class TestDeveloperAgentPromptTemplates:
    """Test DeveloperAgent prompt template handling."""

    def test_load_implementation_prompt_template(self, isolated_agilevv_dir):
        """Test loading implementation prompt template."""
        agent = DeveloperAgent(path_config=isolated_agilevv_dir)

        # Create mock template
        template_dir = Path("verifflowcc/prompts")
        template_dir.mkdir(parents=True, exist_ok=True)
        template_path = template_dir / "implementation.j2"
        template_content = """
        Implement the following design:
        Components: {{ components }}
        Interfaces: {{ interfaces }}
        Generate clean, maintainable code following best practices.
        """
        template_path.write_text(template_content)

        try:
            # Load template
            loaded_template = agent.load_prompt_template("implementation")
            assert "Implement the following design" in loaded_template
            assert "{{ components }}" in loaded_template
        finally:
            # Cleanup
            if template_path.exists():
                template_path.unlink()
            if template_dir.exists() and not list(template_dir.iterdir()):
                template_dir.rmdir()

    def test_load_nonexistent_template(self, isolated_agilevv_dir):
        """Test loading non-existent template returns empty string."""
        agent = DeveloperAgent(path_config=isolated_agilevv_dir)

        template = agent.load_prompt_template("nonexistent_template")
        assert template == ""
