# VeriFlowCC

**VeriFlowCC** is a **Verification Flow Command Center** that integrates **Anthropic’s Claude Opus 4.1 and Sonnet 4 models** to create a **structured, agent-driven software development pipeline**. It enforces the [Agile V-Model](https://aiotplaybook.org/index.php?title=Agile_V-Model) methodology, ensuring rigorous verification and validation (V&amp;V) at each stage of feature development.

When “Verification flow” meets Claude’s intelligence for seamless V&amp;V pipelines.

## Product Requirements Document (PRD)

**Functional Requirements:** The VerifFlowCC CLI tool must enable **structured, agent-driven software generation** with strict adherence to an Agile V-Model workflow. Key functional requirements include:

[ ] **Enforced V-Model Stages:** The system **must enforce sequential phases** – Planning, Design, Coding, Testing, and Validation – for each feature or user story. Progression is gated: *e.g.* code generation cannot begin until requirements and design are documented, and testing must succeed before validation completes. Skipping or reordering stages is disallowed without explicit user action (rollback or restart).

[ ] **Multi-Agent Orchestration:** Utilize a **hierarchical multi-agent approach**.
    - A **Planner agent** (Claude Opus 4.1) orchestrates high-level strategy and planning, the Planner decomposes goals and delegates to sub-agents via a structured workflow.
    - while specialized **Worker sub-agents** (Claude Sonnet 4) execute tasks for each stage (design, coding, testing, review).

[ ] **Claude Code Integration:** Leverage the Claude Code SDK and commands for agent actions. The agents should be able to **autonomously use Claude Code’s capabilities** – reading/writing files, editing code, running tests/shell commands, managing git – via **slash commands or MCP tools**. For example, the coding agent can apply code edits or run `npm test` through an MCP shell tool, with output fed back into the workflow.

[ ] **Context Engineering:** Implement robust context management at each stage. The system uses **structured prompt templates** (via Jinja2) to format inputs for each agent role, ensuring they receive *only relevant context*. It must maintain **persistent memory** across sessions (e.g. a `CLAUDE.md` or `MEMORY.md` file logging decisions, requirements, and design choices). Each agent’s prompt context is isolated to its concern – *e.g.* the testing agent sees the new code and test criteria but not the full design discussion.

[ ] **Plan-Then-Act Workflow:** Enforce a plan-first, then execute paradigm. The Planner agent must produce a clear plan or task checklist covering all V-model steps for the feature. The system will then step through this plan, *one task at a time*, prompting the appropriate sub-agent to act. This ensures the AI “thinks then acts” in a controlled sequence, mirroring best practices like ReACT and SPARC workflows.

[ ] **Checkpointing and Rollback:** The tool must support **reproducible workflows** with the ability to checkpoint state and roll back. Before any major code changes, the CLI should create a checkpoint (e.g. a git commit or file snapshot). If a stage fails – *e.g.* tests do not pass or a validation check is negative – the system can revert to the last checkpoint and allow the Planner (or developer) to adjust the plan. This guarantees a clean state to retry from and prevents compounding errors.

[ ] **Human-in-the-Loop Control:** While automation is key, the developer remains in control. The system should allow the user to review and approve critical outputs at each stage. For example, the Planner’s requirements and design output can be confirmed by the user before coding proceeds. Potentially dangerous actions (file writes, executing code) require user confirmation unless a “trusted” or automation mode is enabled. This aligns with Claude Code’s principle of requiring human approval for impactful actions (like running tools or committing code).

**Non-Functional Requirements:**

* **Modularity & Extensibility:** The architecture must be modular, making it easy to add or modify stages and integrate new tools. Each agent role (planning, coding, testing, etc.) should be implemented as an independent module with clear interfaces, so future workflows (e.g. adding a Security audit stage) can plug in with minimal changes. The design should favor composition of agents/tooling over monolithic prompts.

* **Token Efficiency:** Ensure **minimal token overhead** through context isolation and succinct prompts. Because multiple LLM calls are used, prompts and outputs should be compact and structured. Use **Pydantic schemas** to validate and enforce concise, machine-readable responses (e.g. a JSON plan, a test results report). By giving each agent only the information it needs, the context size per call is reduced, improving cost efficiency. (Opus 4.1 is used sparingly for planning due to its higher cost, while Sonnet 4 handles the bulk of tasks cost-effectively.)

* **Performance:** The pipeline should complete in a reasonable time for a typical feature. Parallelize where possible without breaking the V-model sequence – e.g. running static analysis and security checks concurrently after coding. However, since most V-model stages are sequential by design, latency comes mainly from LLM call time and tool execution time. Opus 4.1’s extended reasoning may take longer per call, so its usage is confined to high-level planning. Caching of intermediate results or prompt embeddings (using Anthropic’s prompt caching up to 1 hour) can be used to speed up iterative runs.

* **Reliability & Robustness:** The system should handle partial failures gracefully. If an agent produces an invalid or incomplete output (e.g. schema validation fails for the Planner’s plan), the orchestrator can detect it and retry or ask the agent for clarification. The workflow should not crash due to an exception; errors are caught and reported to the user with suggestions. Additionally, the design accounts for the possibility of the AI going off-track: observability (below) and human checkpoints help catch and correct such cases early.

* **Observability & Transparency:** Every action the system takes should be observable. Implement **structured logging** for each stage (prompts, model responses, tool outputs) and maintain an **execution trace** that the developer can review. Consider a verbose mode that displays each sub-agent’s prompt and answer, or even a live “dashboard” view of agent actions (as seen in some Claude tool integrations). At the end of a run, provide a **summary report** (e.g. list of tasks done, tests passed, time taken, token usage) similar to Claude Code’s session summary. These features make the agent’s decision process transparent and the results reproducible for auditing.

## Architecture Overview

**Approach:** VerifFlowCC’s architecture follows a **hybrid hierarchical pipeline**. A **Planner (Opus 4.1)** agent handles strategic planning and oversight, while **Worker sub-agents (Sonnet 4)** perform the concrete steps for each V-model phase. The workflow is orchestrated by the CLI application acting as a controller, which implements the V-model as a series of gated stages. This design draws on Anthropic’s recommended patterns for effective AI agents: separating high-level planning from execution, and splitting complex tasks into phases handled by specialized contexts.

Each phase of the Agile V-model is mapped to either an agent role or a specific tool action:

* **Planning Phase:** The Planner agent interprets the user’s objectives (e.g. a new feature request or user story) and produces a structured **plan**. This includes clarifying requirements, defining acceptance criteria, and outlining the subsequent tasks (design, implementation, tests, validation). The plan is emitted in a schema-controlled format (for example, as a checklist or JSON with sections for “Requirements”, “DesignOutline”, “TasksList”) for consistency.

* **Design Phase:** Based on the Planner’s output, a Design sub-agent (using Claude Sonnet 4) creates a **design specification** for the solution. This might be high-level pseudocode, a UML-like description of components, or an architectural plan (C4 sketches) that meets the requirements. The design agent prompt is pre-engineered (via a Jinja template) to include the user story and acceptance criteria from planning, along with any context from previous similar projects (from memory) to encourage reuse of known patterns. The Planner could optionally verify or refine the design before implementation proceeds (acting as a review step).

* **Coding Phase:** The Coding agent (Sonnet 4) generates the actual code based on the approved design. It works directly with the project’s codebase by reading and modifying files. VerifFlowCC leverages Claude Code’s *file-editing commands* and *tool use* capabilities for this stage – for instance, the agent can open relevant files, apply diff patches, or even create new files as needed. The prompt given to the coding agent includes the design spec and any boilerplate or template code available. **Context isolation** ensures only necessary parts of the codebase (e.g. a function signature or schema) are provided, rather than the entire repository. This prevents the model from being overwhelmed by irrelevant context and keeps generation focused. After code generation, the CLI saves the changes (and may commit them to git as a checkpoint).

* **Testing Phase:** Once code is written, the Testing agent uses Sonnet 4 to verify the solution. This can involve generating unit tests (if none provided), but more critically, **executing the test suite or running the program** to validate behavior. The agent is prompted with instructions to run tests (using a tool command) and analyze the results. Through Anthropic’s Model Context Protocol (MCP), the agent can invoke a *shell tool* in a sandboxed manner to run commands like `pytest` or compile and run the software. The output (e.g. test failures or runtime logs) is captured and fed back into the agent’s context. The Testing agent then produces a structured **TestReport** – listing passed/failed tests, errors, or coverage of acceptance criteria. If tests pass, the pipeline moves forward. If not, the workflow enters a *fix cycle*: the failures are given to a Debug sub-agent or back to the Coding agent to fix the issues, and the tests are re-run until green. This loop (test -> fix -> retest) continues under the Planner’s supervision, which may update the plan if new tasks (like fixing a design flaw) are needed.

* **Validation Phase:** In the final phase, the system validates that the implemented feature meets the initial requirements and acceptance criteria. This might be done by the Planner agent (Opus) reviewing all artifacts – requirements, code, tests – and producing a **validation summary**. The summary would confirm each acceptance criterion is satisfied (potentially citing evidence from tests or code) and note any deviations or remaining concerns. Alternatively, a dedicated Validation agent (Sonnet) could cross-check the requirements against the final outputs (this can be as simple as re-reading the user story and having the LLM double-check the code/test for coverage). This acts as a final QA step in the AI’s own language: effectively an AI “UAT” (User Acceptance Test) report. If anything is found lacking, the Planner can decide to iterate again (perhaps logging a todo for the next sprint), otherwise the feature is marked **Done**.

**Sequential Hook Pipeline:** The entire process is implemented as a deterministic pipeline within the CLI. The CLI uses **hook-gated stages** – meaning the system will not invoke a later stage’s agent until the previous stage’s outputs meet predefined exit criteria. For example, the **Planning stage’s exit criterion** might be “a requirements list and plan have been generated and approved by the user”; the **Coding stage’s exit criterion** is “code changes have been applied to the workspace (with user approval for file writes)”; the **Testing stage’s exit** requires “all tests passed”; and **Validation’s exit** could be “Planner confirms all criteria met in summary.” These checks are implemented with code hooks or conditionals in the orchestrator. If a criterion is not met, the pipeline can halt or loop as needed (e.g. on test failure, go back to coding fixes, on validation failure, perhaps revisit design). This **guarantees V-model compliance** and high quality: no step is skipped and any errors are caught in the proper phase.

**Agent Collaboration and Context Isolation:** The Planner and Worker agents collaborate closely but *do not share a single monolithic context*. Instead, the CLI passes only the necessary information to each agent in turn (this is sometimes called *context chunking* or routing). For instance, when the Planner spawns the Coding agent, it may pass along the design spec and a summary of requirements, but not the entire conversation about planning. Similarly, the Testing agent gets the latest code diff and test instructions, but not the full design document except if needed for reference. This **context engineering** approach ensures each agent works within a focused scope, improving accuracy and reducing the chance of the model losing track of details. It also extends effective memory: each agent has its own 100K-token window (for Claude 4) fresh for its task, rather than one agent trying to juggle everything and hitting context limits. The CLI orchestrator is responsible for **transferring the outputs** of one stage into the inputs of the next, which it does using structured data (JSON/YAML or markdown sections) to avoid miscommunication.

**Persistent Memory:** In addition to immediate context hand-offs, VerifFlowCC uses persistent memory files to retain knowledge between sessions. Key artifacts like the feature’s user story, the design decisions, and any important notes (edge cases discovered, out-of-scope items, etc.) are written to a project memory log (e.g. `CLAUDE.md`). Anthropic’s platform supports loading such memory at the start of a session, effectively giving the agent a memory of past work. For example, after validation, the tool might append a snippet to `CLAUDE.md` stating: “Feature X implemented. Design highlights: used approach Y. All tests passed. Outstanding todo: Z.” On the next run (or next feature), the Planner can load this file so it *remembers the context of past features*, preventing redundant explanations and maintaining consistency with prior decisions. This persistent memory mechanism mitigates the forgetting that can happen over long projects and ensures continuity from sprint to sprint.

**Tooling and MCP Integration:** VerifFlowCC is designed to use Anthropic’s Model Context Protocol (MCP) to integrate external tools seamlessly. The CLI can register various tool endpoints (or use Claude Code’s built-in ones) to allow agents to perform actions like file operations, running test commands, checking issue trackers, etc. Notably, a **“shell” MCP tool** is used to execute commands in a controlled environment – for running tests or launching the app – and a **“file edit” tool** is used by agents to apply code changes. By using MCP, these actions appear to the Claude agents as part of their context (they can “ask” to use a tool, and the CLI executes it and returns the result). This makes the workflow more autonomous: the coding agent can itself decide to run a quick syntax check or linter via a tool call, improving its ability to self-correct without always asking the user. All tool usage is done with safety in mind: the CLI sets up each agent’s permissions so that, for example, the Planner agent might not even have access to the file-edit tool (since it should not change code), and the coding agent might have no internet access, etc., following least-privilege principles. Each tool invocation and its result are logged for traceability.

**Opus vs Sonnet roles:** Claude Opus 4.1 is chosen for the Planner because of its superior performance on long, complex tasks and planning-oriented reasoning. It can maintain focus over the entire strategic picture (requirements through validation) and make high-level decisions (e.g. how to break down the feature, whether to refactor existing code or create new modules) with a lower risk of hallucination in extended workflows. Claude Sonnet 4, while slightly less powerful, excels at coding tasks and is more cost-efficient. By using Sonnet for execution (which often involves multiple iterations), we significantly reduce token costs while still achieving high-quality code and test generation. The interplay is thus: one Opus call to plan (and possibly validate at the end), and many Sonnet calls to implement – a balanced use of resources. Both models benefit from the Claude Code features like extended token window and tool use, which VerifFlowCC leverages fully.

Summing up, the architecture ensures that **planning is explicit, execution is segmented, and verification is continuous**. This yields a robust agentic workflow that aligns with Agile’s rapid iteration and the V-Model’s rigor in verification/validation.

## C4 Model Diagrams

### 1. Context Model

At the highest level, VerifFlowCC sits between the **developer** and several external services. The developer interacts with VerifFlowCC through a CLI in the terminal (issuing commands and providing input like feature descriptions). The CLI tool in turn communicates with Anthropic’s Claude API to leverage the Opus 4.1 and Sonnet 4 models as needed. All AI reasoning and code generation happen via these model API calls. The CLI also interfaces with the **local development environment** – reading and writing files in the project’s repository, running test suites, and using system tools (like version control, compilers, etc.). The figure below conceptualizes this context:

* **Developer (User):** Uses the VerifFlowCC CLI on their machine to develop software with AI assistance. They initiate workflows (e.g. “implement feature X”) and supervise the results at each stage.
* **VerifFlowCC CLI:** The tool itself, running locally. It orchestrates the multi-agent workflow and serves as the bridge between the AI models and the developer’s local system. It sends prompts to Anthropic’s Claude service and receives code/plan outputs, applying them to the local codebase or presenting them to the user.
* **Anthropic Claude API:** Cloud service providing access to Claude Opus 4.1 and Sonnet 4 models. VerifFlowCC sends structured prompts and tool-use commands to this API (using the Claude Code SDK), and obtains the model’s responses (plans, code, test analysis, etc.). The API can also handle tool calls (via MCP) by pausing the model’s output and awaiting the tool results.
* **Local Environment (Code Repository & Tools):** This includes the project’s source code, test files, and any relevant data on the developer’s machine. VerifFlowCC reads from and writes to these files as directed by the AI (with permission). It also includes system tools like the shell, compilers, test runners, and git. The CLI may invoke these directly or via Claude’s tool-use interface. Checkpoints (such as git commits) are stored here, and any persistent memory files (e.g. `CLAUDE.md` log) reside in the repo.

In summary, the context diagram shows the developer driving VerifFlowCC, which in turn orchestrates cloud AI models and local tools to produce software artifacts. This central position of the CLI means it must carefully manage inputs/outputs between the human, AI, and system – which is exactly what the following design details ensure.

### 2. Container Model

The container view zooms into VerifFlowCC and its major runtime components. As a CLI application, VerifFlowCC runs as a single OS process on the developer’s machine, but it can be thought of as composed of several logical “containers” or services:

* **CLI Orchestrator Process:** The main Python (or Node, etc.) process that runs VerifFlowCC. This container encompasses the overall control flow – it parses CLI commands, manages state, and coordinates all other components. Within this process, we can delineate sub-containers (modules) for planning, execution, and integration (detailed in the Component Model). The orchestrator is what implements the state machine of the V-model pipeline, ensuring each stage is invoked in order.

* **Anthropic Claude Service (external):** Although external, we depict it as a distinct container since it provides critical functionality. The Claude service container comprises two model endpoints: **Opus 4.1** (used via the Claude Code API for planning) and **Sonnet 4** (for sub-agents). The CLI orchestrator communicates with this container over the network (HTTPS API calls). Under the hood, Anthropic’s service manages the model invocation and, if a tool command is issued in the prompt, it can call back to our CLI via an MCP connection (or rely on our CLI to simulate the tool result). For design purposes, we assume the CLI actively handles tool requests rather than the cloud service calling back.

* **Local Tooling/Environment:** This includes the local **File System** (the project codebase) and **Shell/OS**. We treat it as a container to highlight an interface boundary: VerifFlowCC interacts with the filesystem to read or write code and uses the shell to execute tests. This container is accessible to the CLI process through standard libraries (e.g. Python’s `open()` for files, `subprocess` for shell). In some cases, these actions might be mediated by the Claude agent (via slash commands that our CLI intercepts). This container also includes the **Version Control System** (e.g. git) used for checkpointing: the CLI either calls git CLI commands or uses a git library to commit changes at checkpoints and rollback if needed.

* **Persistent Storage (Memory Packs):** A small but important container is the persistent memory storage, such as the `CLAUDE.md` file or a database of session summaries. This is shown separately to emphasize that even after the CLI process ends, this data remains for future runs. It is typically just a file in the repository (or a hidden folder for tool-specific data). The CLI orchestrator’s first step on startup is to load relevant memory (e.g. open `CLAUDE.md` and include its contents in the Planner’s context). This storage might also contain configuration (like templates, schema definitions, etc., if not baked into the code).

The relationships are as follows: The **CLI Orchestrator** container sends requests to the **Claude Service** (and awaits responses) for any AI-driven tasks. It also reads/writes to the **Local Env** for applying changes or gathering context (like reading a code file to include in a prompt). The **Persistent Memory** is accessed by the orchestrator at session boundaries – load at start, update at end of stages or session. If we consider extending this to a team setting, one could imagine the CLI pushing final code to a remote repo or reporting status to a project tracking system (via MCP connectors), but those are beyond the core scope.

In essence, at the container level we have: **Local CLI app**, **Remote AI service**, and **Local system**. The design cleanly separates concerns: the heavy AI computation and agent logic runs in Claude’s cloud container, the authoritative source of truth (code, tests) resides in the local container, and our CLI bridges the two with orchestrated logic.

### 3. Component Model

Breaking down the internals of the VerifFlowCC CLI, the following are the primary components (modules and their responsibilities):

* **Command-Line Interface Module:** This handles user interaction. It parses CLI commands/arguments (for example, the user might run `veriflowcc plan feature.yaml` or just launch an interactive mode). We might implement this using a library like Python’s Click or Typer for a polished experience. The CLI module prints outputs to the console in a user-friendly way (e.g. streaming model responses or summarizing results) and collects any user confirmations (like “Proceed to coding? (Y/N)”). It’s essentially the UI layer of the tool.

* **Workflow Orchestrator:** The core engine implementing the V-model state machine. This component coordinates the sequence of agent calls and tool invocations. It contains the logic for moving from Planning -> Design -> Coding -> Testing -> Validation, including the **hook gating** checks between stages. The orchestrator invokes the appropriate agent module at each step, monitors the outputs, and decides the next step (including error handling branches). It’s also responsible for calling the checkpoint/rollback component when needed. In implementation, this might be a Python class managing the “conversation state” and calling subroutines for each stage.

* **Planner Agent Module:** This component encapsulates everything needed to interact with the **Opus 4.1** Planner agent. It likely includes a **prompt template** for planning (e.g. a Jinja template that takes in the user story and context and asks for a plan) and a Pydantic **schema** definition for the expected plan structure. The module handles sending the prompt to Claude (via the SDK) and validating/parsing the response into a Plan object. If the response doesn’t fit the schema (e.g. missing a section), the module can detect that and possibly request a correction or fill defaults. The Planner module also contains logic to refine the plan (for instance, if the user says “adjust requirement X,” it can modify the plan accordingly).

* **Design Agent Module:** Similar to the above, but for the design stage (likely using Sonnet 4). It has a prompt template that takes in requirements from the Planner and outputs a detailed design or pseudocode. It might also use a schema or structured format (for example, a JSON with keys like “ArchitectureDiagram: (link)” or “Pseudocode: ...”). The design might be free-form text as well (which the coding agent can then use directly). This module’s output is primarily used as input for coding, but it’s also shown to the user for confirmation.

* **Coding Agent Module:** This is a critical component using Claude Sonnet 4 to generate and modify code. It includes templates for prompts that likely combine instructions + relevant code context. For example, if implementing a specific function, the template might pull in the function’s signature or file contents (up to a limit) and ask the agent to implement the body according to the design. This module manages the iterative edit loop: it might apply an edit suggested by Claude, then verify the diff, and possibly ask Claude to explain it or refine it. It interacts with the **File Manager** sub-component to actually write changes. We also integrate Claude Code’s native abilities here – e.g. using `/edit` or providing a unified diff for the agent to output. The coding module thus acts as the hands of the AI, directly affecting the codebase.

* **Testing Agent Module:** This component handles both test generation (if needed) and test execution analysis. It prepares prompts for Sonnet that include either a request to *generate tests* (given the code and requirements) or to *run and verify tests*. In the latter case, it will likely instruct Claude to use a tool like `/run` or an MCP command to execute tests. The Testing module might split responsibilities: one part to generate or ensure tests exist (perhaps creating a simple test suite if none), and another part to run the tests and parse the results. The parsing can be done by Claude (the agent reads the tool output and summarizes errors) or by the CLI capturing the output and then feeding it into the agent prompt (to avoid the agent hallucinating results). Either way, the output is a *TestReport* data structure, which the module validates (e.g. a Pydantic model with fields like `passed: bool`, `failed_tests: list` etc.). If tests fail, this module communicates with the orchestrator to trigger the fix loop.

* **Validation/Review Agent Module:** Optionally, a module for the final validation stage (this could be combined with Planner or Testing, but conceptually distinct). This uses either Opus (for a holistic review) or Sonnet (for a focused check) to double-check everything. It might, for example, re-read the user story and then inspect the code or run a final end-to-end test scenario. Implementation-wise, this could be just a final prompt to the Planner agent: “Validate that the following acceptance criteria have been met…”. The output would be a *ValidationReport* (which could simply be a confirmation message or a list of verified points). Since validation in V-model often involves user acceptance, this step might produce output for the user to review, effectively bridging into a manual sign-off.

* **Context Manager & Memory Handler:** This cross-cutting component is in charge of assembling the correct context for each agent and managing the persistent memory. It loads the **context pack** (project background, coding conventions, CLAUDE.md memory) at the start and makes sure each agent prompt injects the relevant parts. For example, before calling the Planner, the context manager gathers project-wide notes (from CLAUDE.md), and before calling the Coding agent, it may retrieve the specific file content needed. It also handles summarizing and storing new information: after each stage, it might update the persistent memory (e.g., after planning, store the final requirements in CLAUDE.md for future reference; after coding, store a summary of implementation decisions, etc.). This component ensures **each agent “sees” only what it needs** and that important knowledge is not lost when moving between agents or sessions. In code, this might involve a series of Jinja templates and helper functions to load files or query an embedding store to find relevant past data.

* **File Manager & Checkpointer:** Responsible for all file I/O and version control operations. When the coding agent produces changes, this component takes the diff or file content and applies it to the workspace (with user approval). It also commits the changes to git if a checkpoint is due, tagging the commit with a message (like “Implemented Feature X - codegen checkpoint”). If a rollback is triggered, this module either performs a `git reset` or restores files from backup copies made earlier. It can also stash changes for safety during test runs. Additionally, the File Manager enforces any *write protections* – for instance, if an agent tries to edit an unauthorized file, this component will block it (this ties into the allowlist of tools given to each agent). In implementation, this could use GitPython or simply shell out to git. It also logs file changes for the observability module.

* **Logging & Observability Module:** This handles structured logging of events and providing feedback to the user. It records each prompt sent to Claude (possibly in a condensed form), each response, and each tool action along with timestamps and token usage. In interactive mode, it can display some of this in real-time (like showing the planning checklist as it’s generated, or printing test results live). It can also produce the final **execution summary** – for example, “Plan generated with 5 tasks, Code written to 3 files, 10/10 tests passed, Time taken 2m30s, Tokens used \~50k”. Internally, this might be realized by Python `logging` with JSON output that could be later analyzed, or even a small web server that streams logs to a browser (for a GUI dashboard). The key is that this component makes the pipeline’s operation transparent and debuggable. If an error occurs (say the coding agent didn’t follow instructions), the logs and stored prompts help diagnose where things went wrong.

All these components work in concert within the single CLI application process. The **Claude Code SDK** is used by the agent modules to interface with Claude’s API (likely providing convenient methods to send prompts, use tools, and handle model streaming). The **Jinja templates** are resources used by context manager and agent modules to ensure consistency in how prompts are formatted. The **Pydantic models** provide a contract for inter-module data exchange – e.g. the orchestrator expects a `Plan` object from the Planner module, which Pydantic will construct after validating Claude’s raw output.

This component architecture ensures **high cohesion within stages and loose coupling between them**: each agent module is focused on its phase, and the orchestrator plus context manager tie them together. The design is both **modular and extensible** – new agent roles or tools can be added by adding new modules and adjusting the orchestrator’s sequence, without altering the core logic of existing stages.

## Sequence Flow of V-Model Workflow

The following sequence diagram outlines how a typical VerifFlowCC session would proceed, from the CLI command through each agent interaction and tool invocation, following the V-model stages:

1. **User Initiates Feature Workflow:** The developer runs the CLI (for example: `veriflowcc new-feature "As a user, I want to ..."`). The CLI parses the input (perhaps a user story or spec) and begins a new session. The Logging module starts a log, and the Context Manager loads persistent context (CLAUDE.md, etc.) relevant to this feature.

2. **Planning (Opus) – Requirements & Task Plan:** The orchestrator calls the Planner agent module with the user’s feature description and loaded context. A prompt (using the planning template) is sent to Claude Opus 4.1, requesting a structured plan (including clarified requirements and a list of development tasks). The model’s response, for example, might be:

   ```yaml
   Requirements:
     - "User can do XYZ..."
     - "Edge case ABC handled..."
   DesignOutline:
     - "Use MVC pattern with ... (details)"
   TaskList:
     1. Design the XYZ module (Agent: Design)
     2. Implement feature in code (Agent: Coding)
     3. Write/Update tests for XYZ (Agent: Testing)
     4. Run tests and fix issues (Agent: Testing/Coding)
     5. Final review against criteria (Agent: Validation)
   ```

   The Planner may also include acceptance criteria explicitly. The CLI receives this plan, validates it (schema check), and presents the high-level summary to the user. The user is prompted to confirm or adjust requirements if needed. Once accepted, the workflow proceeds. (If the user requested changes, the Planner could be re-invoked to update the plan).

3. **Design (Sonnet) – High-Level Solution:** The orchestrator now invokes the Design agent (Claude Sonnet 4). It constructs the design prompt with instructions like *“Create a design for the feature based on these requirements”* and may include the **DesignOutline** from the plan if one was provided, or ask the agent to elaborate it. The Design agent returns a more detailed design artifact, for example:

   * A markdown section with **Architecture** (describing new modules or changes to existing ones).
   * Possibly a simple diagram description (which could be later rendered by the user).
   * **Pseudocode** or interface definitions for new components.
     This output is captured by the CLI. The orchestrator (and possibly the Planner agent, if we involve it here) quickly verifies that the design covers the requirements. The user can review this design; once they are satisfied (or after an auto-approval in non-interactive mode), VerifFlowCC moves on.

4. **Coding (Sonnet) – Implementation:** The CLI now triggers the Coding agent. For each task that involves coding (the plan may have broken it into sub-tasks or files), the orchestrator prepares a prompt for the coding agent. For instance, if the first coding task is “Implement the XYZ module,” the CLI will load the skeleton or relevant file context (e.g. if the file exists or needs creation) and feed that, along with the design snippet for XYZ, to the agent. The coding prompt might say: *“Implement the following function according to the design. Here is the current file content (if any) and the design details…”*. Claude Sonnet then outputs code changes. In Claude Code, typically the AI can output a unified diff or just the modified code; our CLI can handle either. Suppose it outputs a diff for `src/xyz.py`. The File Manager component applies this diff to the local repo (after optionally asking the user to approve the changes). The CLI may show the user the diff for transparency.

   * The coding may involve multiple steps; the agent might not get everything perfect on first try. The CLI can have the agent re-read the file and refine if needed (for example, if a function call is left undefined, the CLI can prompt “Fill in the implementation for that function as well”). This is an inner loop within the Coding stage, though in many cases a single agent pass per file could suffice.
   * Once code for all planned tasks is generated and applied, the CLI creates a checkpoint commit “Feature XYZ – code implemented”. This corresponds to the end of the **Coding phase**.

5. **Testing (Sonnet) – Verification Loop:** Now the orchestrator enters the Testing phase. It invokes the Testing agent with a prompt like: *“Run the test suite and report any failures”*. The agent knows (from prompt instructions and Claude Code’s tool context) that it can execute shell commands. It produces a special output instructing the CLI to run `pytest` (for example). The CLI sees this and executes the tests in the local environment. Suppose some tests fail or there are runtime errors; the CLI captures the output logs. It then feeds those logs back into a new prompt to the Testing agent: *“The tests have failures (see below). Analyze these and suggest what parts of the code might be wrong or what needs fixing.”*. Claude Sonnet returns an analysis, perhaps pointing out, *“Test X failed at line 42, likely because condition Y isn’t handled. To fix, adjust the logic in function Z to account for Y.”*. The orchestrator then routes this information to the Coding agent (or possibly directly triggers a fix if simple) – effectively initiating a **debugging sub-cycle**.

   * **Debug/Fix sub-cycle:** The Coding agent is called again with the specific issue: *“Fix the bug causing test X to fail. Here is the error and Claude’s diagnosis…”*. The agent modifies the code (maybe a small edit in one file). The change is applied, and a new commit (or stash) is made for the fix. Then the tests are re-run. This loop continues until the Testing agent reports all tests pass (or the Planner decides the failing tests indicate a requirement change, which would be a larger scope issue). In a successful scenario, after a couple of fixes, the tests pass. The Testing agent finally outputs something like *“All 25 tests passed. Coverage is 98%. No errors remaining.”* The orchestrator moves on.

6. **Validation (Opus) – Final Review:** With all automated tests green, the CLI performs the last validation step. It can re-engage the Planner (Opus) for a comprehensive review: *“Double-check that the implemented solution meets all acceptance criteria: (list criteria). Verify code quality and completeness.”*. Because Opus has the broad context, it can summarize: *“Criterion A: met (as evidenced by …); Criterion B: met; … All acceptance criteria appear satisfied. Additionally, code was refactored for clarity in module X. Recommend verifying performance in production.”*. This response serves as an AI-generated *validation report*. The CLI presents this to the user. In most cases, this will be the end: the feature is done and verified. If the Planner had raised any concern (perhaps noticing a missing edge-case that tests didn’t cover), the user might decide to address it (that could spawn a mini-cycle again). Otherwise, the workflow is complete.

7. **Completion and Summary:** The CLI finalizes the session. The persistent memory (`CLAUDE.md`) is updated with a brief log of what was done – for example, the user story and a note that “Feature implemented and validated on 2025-08-07”. The Logging module then prints an execution summary to the console, e.g.: *“Plan: 5 steps. Code: 3 files changed. Tests: 25 passed, 0 failed. Time: 2 minutes 45 seconds. Tokens used: \~60k (cost \$0.18).”*. It may also suggest next steps (like “You can run `veriflowcc review` to see the diff, or `git push` to upload changes.”). With that, the CLI returns to await the next command or exits. The developer can confidently mark the user story as done, with traceability of all the steps taken.

Throughout this sequence, **observability** is maintained: each agent’s actions (prompts, code diffs, test outputs) were logged or displayed, so the developer could intervene if something looked off at any point. The strict stage gating ensured that, for example, we didn’t end up writing code without a plan, or deploying untested code. The combination of automated reasoning and human oversight yields a reliable yet efficient development cycle.

## File Structure and Toolchain

A well-organized project structure will aid in maintaining the VerifFlowCC codebase. Below is a recommended file/directory layout and the technologies used:

```
VeriFlowCC/                   # Project root (could be a pip package)
├── verifflowcc_cli.py        # Entry point for the CLI (argument parsing, main loop)
├── agents/
│   ├── planner_agent.py      # Opus 4.1 Planner agent logic (prompt templates, schema, API call)
│   ├── design_agent.py       # Sonnet 4 Design agent logic
│   ├── coding_agent.py       # Sonnet 4 Coding agent logic
│   ├── testing_agent.py      # Sonnet 4 Testing agent logic (including test execution handling)
│   └── validation_agent.py   # Sonnet/Opus Validation agent (or this could be part of planner_agent)
├── prompts/
│   ├── planner_prompt.jinja  # Jinja2 template for planning prompts (requirements & plan checklist)
│   ├── design_prompt.jinja   # Template for design stage
│   ├── coding_prompt.jinja   # Template for coding stage (e.g. with placeholders for file context)
│   ├── testing_prompt.jinja  # Template for running tests or analyzing test output
│   └── validation_prompt.jinja # Template for final validation queries
├── schemas/
│   ├── plan_schema.py        # Pydantic models for structured outputs (Plan, Task, etc.):contentReference[oaicite:125]{index=125}
│   ├── code_schema.py        # (If needed) models for code change or diff representation
│   └── test_schema.py        # Model for test results (pass/fail, failures list) etc.
├── core/
│   ├── orchestrator.py       # Implements the V-model workflow control (stage transitions, gating logic)
│   ├── context_manager.py    # Handles context assembly, memory loading, and prompt injection:contentReference[oaicite:126]{index=126}
│   ├── file_manager.py       # Handles file operations, git checkpoints, rollbacks:contentReference[oaicite:127]{index=127}
│   ├── tool_interface.py     # Abstractions for MCP tool calls (shell execute, etc.)
│   └── logger.py             # Structured logging and event tracing:contentReference[oaicite:128]{index=128}
├── memory/
│   └── CLAUDE.md             # Persistent memory file (project journal) loaded into contexts:contentReference[oaicite:129]{index=129}
├── tests/                    # (Optional) internal tests for VerifFlowCC itself
└── pyproject.toml/setup.py   # Project metadata, dependencies (Claude SDK, Pydantic, Jinja2, etc.)
```

**Toolchain and Technologies:**

* **Claude Code SDK:** VerifFlowCC will use Anthropic’s Claude Code SDK or API client to interface with the Claude models. This SDK provides methods to send prompts, perhaps specialized functions for agentic operations (like starting a Claude Code session on a directory). It also might handle streaming responses and tool usage under the hood. Using the SDK simplifies authentication and ensures compatibility with Claude’s latest features (like the code execution tool and prompt caching). The SDK is likely Python-based (given Claude Code’s CLI is Python) and would be listed as a dependency.
* **Jinja2:** All prompt templates are defined with Jinja2 for clarity and reuse. For example, `planner_prompt.jinja` might look like:

  ```jinja
  Please read the user story and acceptance criteria below, then output a YAML plan.

  User Story:
  {{ user_story }}

  Acceptance Criteria:
  {% for crit in acceptance_criteria %} - {{ crit }}{% endfor %}

  Plan Format:
  (Provide "Requirements", "DesignOutline", and "TaskList" as shown...)

  ```

  At runtime, the context\_manager fills in these templates with the actual user input and context. Using templates enforces consistency in how we ask things, making it easier for the model to follow our expected format.
* **Pydantic:** We utilize Pydantic for defining data models corresponding to the structured outputs and for validating AI responses. For instance, `PlanSchema` might have fields: `requirements: List[str]`, `tasks: List[Task]`, etc. After getting the Planner’s YAML/JSON output, we attempt to parse it via Pydantic. If it fails (due to model deviating from format), we can either correct the prompt or apply a retry logic (possibly asking the model to output JSON explicitly). Pydantic helps convert the free-form text into reliable Python objects that the orchestrator can use (e.g. iterating over tasks easily). This adds a layer of robustness given the unpredictability of LLM outputs.
* **Shell Integration (MCP & Subprocess):** For running tests or other commands, the toolchain includes either using **subprocess calls** (Python’s `subprocess.run`) or routing through Claude’s MCP. Since Claude Code supports a built-in bash tool, we could let the model invoke it. However, to keep more control, VerifFlowCC might intercept requests to run commands and run them itself. The `tool_interface.py` abstracts this: it could parse a special token or format in Claude’s output indicating a tool use (e.g. the model might respond with `<RUN> pytest</RUN>` or some agreed pattern), then the orchestrator (via tool\_interface) executes that command locally and captures output to feed back. This approach is similar to how an agent framework would operate, ensuring the AI’s desire to use a tool is fulfilled in a safe manner. We will ensure that these calls are sandboxed (for example, using a testing database, not production DB, when running integration tests, etc.).
* **Git for Checkpointing:** The file\_manager uses Git for version control. It will initialize a repo if not already (or integrate with an existing one). Before major stages (end of coding, after fixes), it commits changes with a message. For rollback, it uses `git reset --hard HEAD~1` or keeps a pointer to the last good commit and resets to it. This leverages a well-tested tool (git) for preserving and restoring state, instead of writing our own backup system. Additionally, using git means the developer can inspect diffs and history outside the tool, and it dovetails into their normal workflow (they can push the commits to remote, etc.).
* **Configuration & Extensibility:** The CLI could use a config file (e.g. `verifflowcc.config.yml`) to specify settings: which model versions to use (Opus vs Sonnet), API keys, toggling auto-approval, etc. Tool integrations (like additional MCP servers for Jira, Figma, etc.) could also be configured here for future extension, but by default the tool focuses on the code/test scope.

**Dependencies:** Aside from the Claude SDK, Jinja2, Pydantic, and Git (which is external), we’d use standard libraries. If needed, we might use `rich` library to pretty-print things in the terminal (like coloring diffs or rendering the planning checklist with checkboxes for fun). The overall footprint should remain lightweight, as simplicity is a priority.

This file structure and tech stack ensure that the project is maintainable: prompt templates are separate from code, agent logic is separated by concern, and everything converges in the orchestrator. Developers contributing to VerifFlowCC can easily find where to tweak a prompt or adjust how testing is executed, etc., without wading through one giant script. The use of widely adopted libraries (Jinja2, Pydantic) aligns with the goal of clarity and reliability in how we handle LLM I/O.

## Observability and Checkpointing Features

VerifFlowCC includes robust observability to make its autonomous processes as transparent and controllable as possible:

* **Structured Logging:** Every significant event is logged with details. This includes: prompts sent to the model (we might log a trimmed version or just the intent, to avoid huge logs), model responses, any tool commands executed and their outputs, and decisions made by the orchestrator (e.g. “Tests failed, triggering fix loop”). Logs are timestamped and categorized by stage. Using a structured format (e.g. JSON lines or YAML) for logs allows easy filtering – for example, a user could grep for `"stage": "TESTING"` to see all test-related actions. These logs could be written to a file (`verifflowcc.log`) for later review, or even just printed to console in debug mode. The key is **traceability** – one should be able to trace how a code change came to be by reading the logs, which is important in collaborative environments or when diagnosing an AI mistake.
* **Live Progress Feedback:** During a run, the CLI provides live feedback to the user. For instance, as the Planner agent produces a checklist, the CLI could render it interactively (Claude Code’s UI shows a live checklist of actions in progress; in CLI we might simulate that by printing each task as it comes). During coding, we might show which files are being edited. During testing, we can stream test output or at least a progress bar. This keeps the developer engaged and aware of what the AI is doing, avoiding a “black box” feel. If something looks wrong (say the plan misses a requirement), the user can interrupt the process (Ctrl+C or a specific abort command) and intervene.
* **Session Summary:** At the end of each session (or feature run), VerifFlowCC prints a **session summary** similar to Claude Code’s own summary. This includes the number of Claude API calls made, total tokens consumed (if available from the API usage info), time taken, and a recap of outcomes: e.g. “Plan generated, 3 files changed, all tests passed.” It may also include cost estimation (since we know Opus and Sonnet token pricing). For the developer, this provides immediate insight into efficiency, and for teams, it could be logged to track usage over time. The summary might also note if any steps required manual intervention (e.g. “User adjusted requirement 2 in planning stage”) as a mini post-mortem.
* **Checkpoints & Rollback Mechanism:** As described, the tool creates checkpoints using git commits at key points. This not only allows rollback but also serves as an audit log of changes. Each commit message can reference the task from the plan it addressed (e.g. “Task 3: Implemented X, tests failing on Y”). If something goes awry and the user wants to undo the AI’s changes, they can use the CLI to rollback (e.g. `veriflowcc rollback 1` to go to the previous checkpoint) or do it manually via git. The orchestrator itself will invoke rollback automatically on critical failures: for example, if after multiple fix attempts a test still fails, it might decide to rollback to the pre-coding state and ask the Planner to reconsider the approach (thus ensuring a bad partial implementation doesn’t pollute the codebase). The tool will clearly inform the user of such rollbacks: “Tests failed repeatedly, rolled back to last known good state. Please revise the requirements or design.” This prevents the codebase from entering a broken state.
* **Error Handling and Alerts:** If the AI produces an output that triggers a safety rule or is invalid (say the coding agent tries to delete a critical file, or the plan is missing sections), the system will log a warning and either fix it or ask for user guidance. We will integrate Claude’s own safety signals if available (Anthropic models might return a flag if something unsafe was attempted). For instance, if a tool execution returns an error (like a test command crashes), that’s logged and presented to the AI for analysis, but also surfaced to the user in case it’s an environment issue (like “could not find `pytest` command”). The user is never left guessing what went wrong – the tool should always either handle it or explain it.
* **Analytics & Metrics (future):** We can imagine adding optional telemetry, such as measuring how many iterations the fix loop usually takes, or success rate of tests first-pass. For now, these are out of scope, but the structured logs would allow computing such metrics. Another observability enhancement could be an interactive trace visualization (like outputting a Mermaid sequence diagram of what happened, generated from the logs). Given our comprehensive logging, such features could be built on top later.

With these observability features, VerifFlowCC aims to be **developer-friendly and trustworthy**. The user should feel in control: they can see what the AI is planning and doing at each step, and they have mechanisms to intervene (approve, rollback, adjust inputs). The thorough logging and memory of decisions also means the process is **auditable** – important in production scenarios where one might need to review why the AI made a certain change (the decision will be in CLAUDE.md or in the commit message, etc., because we prompted it to explain as it goes). This transparency addresses one of the main concerns with agentic coding: by not hiding the agent’s chain-of-thought, we avoid the “opaque automation” problem and instead create a partnership between the human and the AI, with the tool as the facilitating framework.

## Conclusion

The VerifFlowCC CLI architecture combines the strengths of **Agile’s adaptability** with the **V-Model’s discipline**, using AI agents as intelligent collaborators in the development process. By employing a Planner-Orchestrator (Claude Opus 4.1) to strategize and specialized Worker agents (Claude Sonnet 4) to execute each development phase, the system ensures complex software tasks are broken down, tackled methodically, and rigorously verified. We applied *context engineering* throughout – from Jinja-crafted prompts and isolated agent contexts to persistent memory logs – to keep the models focused and informed without overrunning their context limits. The result is an architecture that is **modular, extensible, and efficient** in token usage, as each agent sees just what it needs and nothing more.

Crucially, VerifFlowCC never loses sight of the developer’s oversight: every action is logged and check-pointed, and no code goes untested or unvalidated before moving forward. This enforces a quality bar that gives developers confidence in the AI’s contributions. Yet, the process remains *agile* – by keeping iterations small (feature-focused) and allowing mid-course corrections (via user input or automated rollback), we avoid long cascades of errors and ensure the AI can quickly adapt to changes in requirements or design discoveries.

In summary, the designed architecture meets the project’s goals: it provides a **structured, agentic software generation** workflow that is **reliable and observable**, marries advanced Claude capabilities with practical engineering principles, and lays a foundation that can grow with future needs (new agents, more tools, bigger projects) thanks to its clarity and modularity. VerifFlowCC can become a powerful assistant for developers building production software, handling the heavy lifting of coding and verification while developers steer and supervise – ultimately speeding up development cycles without sacrificing quality.

**Sources:** The architectural concepts and best practices referenced here draw upon Anthropic’s documentation and community insights on Claude Code’s design and agent orchestration, ensuring that VerifFlowCC is built on proven foundations for effective AI-driven development. The emphasis on context management, tool integration, and sequential control are informed by state-of-the-art discussions in the AI dev community, aligning VerifFlowCC with the cutting edge of AI-assisted programming workflows.
