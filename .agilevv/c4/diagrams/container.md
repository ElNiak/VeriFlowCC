### 2. Container Model

The container view zooms into VerifFlowCC and its major runtime components. As a CLI application, VerifFlowCC runs as a single OS process on the developer’s machine, but it can be thought of as composed of several logical “containers” or services:

- **CLI Orchestrator Process:** The main Python (or Node, etc.) process that runs VerifFlowCC. This container encompasses the overall control flow – it parses CLI commands, manages state, and coordinates all other components. Within this process, we can delineate sub-containers (modules) for planning, execution, and integration (detailed in the Component Model). The orchestrator is what implements the state machine of the V-model pipeline, ensuring each stage is invoked in order.

- **Anthropic Claude Service (external):** Although external, we depict it as a distinct container since it provides critical functionality. The Claude service container comprises two model endpoints: **Opus 4.1** (used via the Claude Code API for planning) and **Sonnet 4** (for sub-agents). The CLI orchestrator communicates with this container over the network (HTTPS API calls). Under the hood, Anthropic’s service manages the model invocation and, if a tool command is issued in the prompt, it can call back to our CLI via an MCP connection (or rely on our CLI to simulate the tool result). For design purposes, we assume the CLI actively handles tool requests rather than the cloud service calling back.

- **Local Tooling/Environment:** This includes the local **File System** (the project codebase) and **Shell/OS**. We treat it as a container to highlight an interface boundary: VerifFlowCC interacts with the filesystem to read or write code and uses the shell to execute tests. This container is accessible to the CLI process through standard libraries (e.g. Python’s `open()` for files, `subprocess` for shell). In some cases, these actions might be mediated by the Claude agent (via slash commands that our CLI intercepts). This container also includes the **Version Control System** (e.g. git) used for checkpointing: the CLI either calls git CLI commands or uses a git library to commit changes at checkpoints and rollback if needed.

- **Persistent Storage (Memory Packs):** A small but important container is the persistent memory storage, such as the `CLAUDE.md` file or a database of session summaries. This is shown separately to emphasize that even after the CLI process ends, this data remains for future runs. It is typically just a file in the repository (or a hidden folder for tool-specific data). The CLI orchestrator’s first step on startup is to load relevant memory (e.g. open `CLAUDE.md` and include its contents in the Planner’s context). This storage might also contain configuration (like templates, schema definitions, etc., if not baked into the code).

The relationships are as follows: The **CLI Orchestrator** container sends requests to the **Claude Service** (and awaits responses) for any AI-driven tasks. It also reads/writes to the **Local Env** for applying changes or gathering context (like reading a code file to include in a prompt). The **Persistent Memory** is accessed by the orchestrator at session boundaries – load at start, update at end of stages or session. If we consider extending this to a team setting, one could imagine the CLI pushing final code to a remote repo or reporting status to a project tracking system (via MCP connectors), but those are beyond the core scope.

In essence, at the container level we have: **Local CLI app**, **Remote AI service**, and **Local system**. The design cleanly separates concerns: the heavy AI computation and agent logic runs in Claude’s cloud container, the authoritative source of truth (code, tests) resides in the local container, and our CLI bridges the two with orchestrated logic.
