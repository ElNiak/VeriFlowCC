### 3. Component Model

Breaking down the internals of the VerifFlowCC CLI, the following are the primary components (modules and their responsibilities):

- **Command-Line Interface Module:** This handles user interaction. It parses CLI commands/arguments (for example, the user might run `AgileVerifFlowCC plan feature.yaml` or just launch an interactive mode). We might implement this using a library like Python’s Click or Typer for a polished experience. The CLI module prints outputs to the console in a user-friendly way (e.g. streaming model responses or summarizing results) and collects any user confirmations (like “Proceed to coding? (Y/N)”). It’s essentially the UI layer of the tool.

- **Workflow Orchestrator:** The core engine implementing the V-model state machine. This component coordinates the sequence of agent calls and tool invocations. It contains the logic for moving from Planning -> Design -> Coding -> Testing -> Validation, including the **hook gating** checks between stages. The orchestrator invokes the appropriate agent module at each step, monitors the outputs, and decides the next step (including error handling branches). It’s also responsible for calling the checkpoint/rollback component when needed. In implementation, this might be a Python class managing the “conversation state” and calling subroutines for each stage.

- **Planner Agent Module:** This component encapsulates everything needed to interact with the **Opus 4.1** Planner agent. It likely includes a **prompt template** for planning (e.g. a Jinja template that takes in the user story and context and asks for a plan) and a Pydantic **schema** definition for the expected plan structure. The module handles sending the prompt to Claude (via the SDK) and validating/parsing the response into a Plan object. If the response doesn’t fit the schema (e.g. missing a section), the module can detect that and possibly request a correction or fill defaults. The Planner module also contains logic to refine the plan (for instance, if the user says “adjust requirement X,” it can modify the plan accordingly).

- **Design Agent Module:** Similar to the above, but for the design stage (likely using Sonnet 4). It has a prompt template that takes in requirements from the Planner and outputs a detailed design or pseudocode. It might also use a schema or structured format (for example, a JSON with keys like “ArchitectureDiagram: (link)” or “Pseudocode: ...”). The design might be free-form text as well (which the coding agent can then use directly). This module’s output is primarily used as input for coding, but it’s also shown to the user for confirmation.

- **Coding Agent Module:** This is a critical component using Claude Sonnet 4 to generate and modify code. It includes templates for prompts that likely combine instructions + relevant code context. For example, if implementing a specific function, the template might pull in the function’s signature or file contents (up to a limit) and ask the agent to implement the body according to the design. This module manages the iterative edit loop: it might apply an edit suggested by Claude, then verify the diff, and possibly ask Claude to explain it or refine it. It interacts with the **File Manager** sub-component to actually write changes. We also integrate Claude Code’s native abilities here – e.g. using `/edit` or providing a unified diff for the agent to output. The coding module thus acts as the hands of the AI, directly affecting the codebase.

- **Testing Agent Module:** This component handles both test generation (if needed) and test execution analysis. It prepares prompts for Sonnet that include either a request to *generate tests* (given the code and requirements) or to *run and verify tests*. In the latter case, it will likely instruct Claude to use a tool like `/run` or an MCP command to execute tests. The Testing module might split responsibilities: one part to generate or ensure tests exist (perhaps creating a simple test suite if none), and another part to run the tests and parse the results. The parsing can be done by Claude (the agent reads the tool output and summarizes errors) or by the CLI capturing the output and then feeding it into the agent prompt (to avoid the agent hallucinating results). Either way, the output is a *TestReport* data structure, which the module validates (e.g. a Pydantic model with fields like `passed: bool`, `failed_tests: list` etc.). If tests fail, this module communicates with the orchestrator to trigger the fix loop.

- **Validation/Review Agent Module:** Optionally, a module for the final validation stage (this could be combined with Planner or Testing, but conceptually distinct). This uses either Opus (for a holistic review) or Sonnet (for a focused check) to double-check everything. It might, for example, re-read the user story and then inspect the code or run a final end-to-end test scenario. Implementation-wise, this could be just a final prompt to the Planner agent: “Validate that the following acceptance criteria have been met…”. The output would be a *ValidationReport* (which could simply be a confirmation message or a list of verified points). Since validation in V-model often involves user acceptance, this step might produce output for the user to review, effectively bridging into a manual sign-off.

- **Context Manager & Memory Handler:** This cross-cutting component is in charge of assembling the correct context for each agent and managing the persistent memory. It loads the **context pack** (project background, coding conventions, CLAUDE.md memory) at the start and makes sure each agent prompt injects the relevant parts. For example, before calling the Planner, the context manager gathers project-wide notes (from CLAUDE.md), and before calling the Coding agent, it may retrieve the specific file content needed. It also handles summarizing and storing new information: after each stage, it might update the persistent memory (e.g., after planning, store the final requirements in CLAUDE.md for future reference; after coding, store a summary of implementation decisions, etc.). This component ensures **each agent “sees” only what it needs** and that important knowledge is not lost when moving between agents or sessions. In code, this might involve a series of Jinja templates and helper functions to load files or query an embedding store to find relevant past data.

- **File Manager & Checkpointer:** Responsible for all file I/O and version control operations. When the coding agent produces changes, this component takes the diff or file content and applies it to the workspace (with user approval). It also commits the changes to git if a checkpoint is due, tagging the commit with a message (like “Implemented Feature X - codegen checkpoint”). If a rollback is triggered, this module either performs a `git reset` or restores files from backup copies made earlier. It can also stash changes for safety during test runs. Additionally, the File Manager enforces any *write protections* – for instance, if an agent tries to edit an unauthorized file, this component will block it (this ties into the allowlist of tools given to each agent). In implementation, this could use GitPython or simply shell out to git. It also logs file changes for the observability module.

- **Logging & Observability Module:** This handles structured logging of events and providing feedback to the user. It records each prompt sent to Claude (possibly in a condensed form), each response, and each tool action along with timestamps and token usage. In interactive mode, it can display some of this in real-time (like showing the planning checklist as it’s generated, or printing test results live). It can also produce the final **execution summary** – for example, “Plan generated with 5 tasks, Code written to 3 files, 10/10 tests passed, Time taken 2m30s, Tokens used ~50k”. Internally, this might be realized by Python `logging` with JSON output that could be later analyzed, or even a small web server that streams logs to a browser (for a GUI dashboard). The key is that this component makes the pipeline’s operation transparent and debuggable. If an error occurs (say the coding agent didn’t follow instructions), the logs and stored prompts help diagnose where things went wrong.

All these components work in concert within the single CLI application process. The **Claude Code SDK** is used by the agent modules to interface with Claude’s API (likely providing convenient methods to send prompts, use tools, and handle model streaming). The **Jinja templates** are resources used by context manager and agent modules to ensure consistency in how prompts are formatted. The **Pydantic models** provide a contract for inter-module data exchange – e.g. the orchestrator expects a `Plan` object from the Planner module, which Pydantic will construct after validating Claude’s raw output.

This component architecture ensures **high cohesion within stages and loose coupling between them**: each agent module is focused on its phase, and the orchestrator plus context manager tie them together. The design is both **modular and extensible** – new agent roles or tools can be added by adding new modules and adjusting the orchestrator’s sequence, without altering the core logic of existing stages.
