# Agile V-Model Overview

The V-model is a systems development lifecycle which has verification and validation "built in". It is often used for the development of mission critical systems, e.g., in automotive, aviation, energy and military applications. It also tends to be used in hardware-centric domains. Not surprisingly, the V-model uses a v-shaped visual representation, where the left side of the "V" represents the decomposition of requirements, as well as the creation of system specifications ("definition and decomposition"). The right side of the "V" represents the integration and testing of components. Moving up on the right side, testing usually starts with the basic verification (e.g., unit tests, then integration tests), followed by validation (e.g., user acceptance tests).

Combining agile development with the V-model is not a contradiction. They can both work very well together, as shown in the figure following:

- Agile methods use story maps including epics, themes, features and user stories for logical decomposition. This maps well to the left side of the V

- Continuous Integration / Continuous Test / Continuous Delivery are inherently agile methods, that map well to the right side of the V

- The key assumption is that the V-model is not used like one large waterfall approach. Instead, the Agile V-model must ensure that the sprints themselves will become Vs according to the V-model

There are two options to implement the latter:

1. Each sprint becomes a complete V, including development and integration/test.
   The agile schedule introduces the concept of dedicated integration sprints.

1. One V becomes 2 sprints: one development sprint, one integration sprint

There are pros and cons to both approaches. The complexity and scale of the project will surely play a role in determining the best setup.

For most projects / product teams, it is recommended that development and integration are combined in a single sprint ("v-sprint"). Only for projects with a very high level of complexity and dependencies, e.g., between components developed by different organizations, is it recommended to alternate between development and integration sprints. The latter approach is likely to add inefficiencies to the development process, but could be the only approach to effectively deal with alignment across organizational boundaries.

To illustrate how AgileVerifFlowCC orchestrates work, we describe two scenarios: Sprint 0 (project foundation) and a normal Sprint N (feature development). Each follows the Agile V-model loop, with the tool enforcing the proper sequence of activities and artifact generation.

## Sprint 0 - Foundations and Setup

Sprint 0 is a preparation phase to lay down requirements and architecture before development sprints begin ￼. AgileVerifFlowCC performs the following sequence in Sprint 0 (often automatically upon init unless the user skips it):

1. Collect Vision & Initial Requirements: The user (or the tool, if provided with a brief) inputs the high-level product vision. The Requirements subagent (Product Owner persona) takes this and produces an initial Story Map/Backlog ￼. This means identifying major epics or features and breaking them into tentative user stories. For example, if the project is a to-do list app, it would list epics like “User Accounts”, “Task Management”, “Sharing”, etc., each with some user stories. The subagent ensures each story has at least a short description and acceptance criteria (or notes on definition of done). These are compiled into backlog.md. (Citations: The Agile V-model calls for capturing requirements progressively in a backlog rather than upfront documents; Sprint 0 is typically used to seed this backlog.)

1. Establish High-Level V&V Criteria: In parallel, or as part of the story map, the tool ensures that for each epic there is a notion of how it will be validated. For instance, for each feature it notes any critical quality requirements or compliance needs (e.g. “must handle 1000 users = performance test criteria”). This essentially sets up a traceability matrix concept from the start, linking features to how they will be verified. The Requirements agent might populate a section in backlog.md or a separate testing-strategy.md describing these overall V&V approaches. (This is especially important in AIoT or safety contexts where even agile teams must plan verification strategies early ￼.)

1. Draft Component Architecture: The Design subagent (Architect persona) is invoked next. It reads the list of features and proposes an initial architecture – e.g. a list of components or modules and their responsibilities ￼. It might create a top-level diagram (in text form) or outline: for the to-do app example, it could define a front-end web app, a backend service, a database, etc., plus any external integrations. The architecture is stored in architecture.md. The design agent’s output ensures that “different components will integrate well” and follows any known constraints (the prompt could mention known tech stack choices, e.g. “we will use React for frontend, Python for backend”). This step aligns with the V-model practice of having a system design up front, but scaled to an agile scope. If the project is small, this could be just a simple overview; if complex, it sets up a “component landscape” to be refined each sprint.

1. Setup Project Memory and Repo: The CLI now has enough info to populate the persistent memory. It consolidates important details (maybe a summary of the product vision, key requirements, and architectural decisions) into the CLAUDE.md file for the project. This file acts as a quick reference for Claude in future sessions. For instance, it might import the backlog and architecture files (See @backlog.md for user stories; see @architecture.md for system overview) so Claude can easily access them ￼ ￼. The CLI might run the /init command of Claude-Code to set this up ￼. Additionally, Sprint 0 would initialize the code repository structure (creating empty directories for \<project_dir>/, tests/, etc., possibly even scaffolding a basic project if technology is known – Claude’s developer agent could be asked to generate a “Hello World” baseline). These actions ensure the environment is ready for actual development sprints.

1. Review and Baselining: After these automated suggestions, the user can review the backlog.md and architecture.md produced. AgileVerifFlowCC could open these in an editor or print a summary. The user may adjust things (e.g. rename a feature, clarify a requirement). Once satisfied, the state is considered the baseline. The tool might commit these files to version control (if integrated with git) as the initial commit, labeled “Sprint 0 – initial backlog and design”. This baseline will serve as a reference to track changes in later sprints, enhancing traceability.

Outcome of Sprint 0: a clear set of initial requirements (backlog) and a high-level design, stored in the project, plus the environment configured with Claude’s memory. At this point, the Agile V-model’s left-side is established enough to begin iterative development – we have defined “what to build” and “how to verify it” at a high level ￼ ￼. Stakeholders could sign off on this if needed (in a real scenario), but in our AI-driven flow, this just means the AI has a stable foundation to refer back to.

## Sprint N - Iterative Development Cycle

For each regular sprint (Sprint 1, 2, 3, … N), the process is a microcosm of the V-model – essentially each user story goes through all V-model phases within the sprint ￼. Sprint N (assuming N>0) follows these steps, typically focused on a single user story or a small set of related stories that the team (here, the AI and user) commits to deliver in that sprint:

1. Sprint Planning & Story Selection: The orchestrator (CLI) picks the next ready user story from backlog.md (or the user chooses one via command). This becomes the scope of Sprint N. The CLI may display the story and acceptance criteria to the user for confirmation. For example: Sprint 3: Implement “Share To-Do List” feature (story #5). Acceptance: user can share a list via email, and the recipient can view it. The tool ensures any prerequisites (other stories or tasks) are completed or flags if not. Planning is kept lightweight – since each sprint should deliver a potentially shippable increment, we do them one by one. (If multiple stories are taken in one sprint, the tool would loop through this process for each or handle them in parallel threads, but initially we assume one story per sprint for clarity.)

1. Requirements Refinement (Left-side V): The Requirements subagent is invoked to refine the selected story. Even though the story exists from the backlog, now is the time to elaborate details. The agent may ask questions or add detail: e.g., “What does the email content look like? Are there security requirements when sharing?” It updates the acceptance criteria if needed or notes any non-functional requirements related to this story (e.g. performance, security considerations to test). The output is an updated story in backlog.md with full details. If the backlog was just a summary, now it becomes a concrete specification for this sprint. This step ensures we have a solid requirement to verify later. (If the user is in interactive mode, the tool might show the new acceptance criteria and ask for approval or additional input, mimicking a grooming session.)

1. Design & Planning: Next, the Design subagent takes the refined requirement and plans the implementation approach. This might involve updating the architecture: for instance, adding a new module or API endpoint for the “share” feature. The design agent could produce a short design proposal: which components will be affected, any new data structures, and how the feature will integrate. It could also outline the tasks for implementation (essentially, a mini plan that the Developer agent can follow, akin to an internal checklist). For example: “To implement sharing, we will add a ShareController with an endpoint, update the UI to include a ‘Share’ button, and send emails via an EmailService. We must also add a permission so only the list owner can share.” This design note is added to architecture.md or a sprint-specific notes file. The orchestrator can enforce that the design covers all acceptance criteria (perhaps by prompting, “Does the design address how we’ll verify the email was received?” to which the agent might add a note to send a confirmation email which will be tested). This step corresponds to the upper left of the V and its alignment with verification plans on the right ￼ ￼ – we’re essentially pairing each requirement detail with a design approach and noting how it will be tested later.

1. Implementation (Code) – Bottom of the V: The Developer subagent now implements the feature according to the design. The orchestrator might break this down: it could instruct the Developer agent step-by-step (especially if using a plan-then-act approach ￼ ￼). For example, first “create ShareController and EmailService classes”, then “update UI with share button”, etc. In an automated run, the Developer agent might attempt the whole implementation in one go, but we encourage iterative development: the CLI could loop: generate code -> run basic tests -> refine code. Claude-Code’s toolset allows the agent to write code to files and even run it if needed. With hooks, we can automatically run formatters or linters after file edits ￼ to ensure code quality. During this phase, memory and context are crucial – the Developer agent uses the CLAUDE.md memory (which includes coding standards or project context) to maintain consistency, and it was given the relevant design and requirements context so it doesn’t forget what to build. If at any point the agent seems to drift (common in long AI sessions), the orchestrator can intervene by re-injecting the key points or using Claude’s /focus or /compact commands to manage context ￼. The outcome is one or more modified/created source files implementing the story. The CLI may generate a diff or summary of changes for the user to review (useful in interactive mode or logging).

1. Verification (Testing) – Right side of V: Once the code is written, the QA/Test subagent takes over to verify that the implementation meets the requirements. It generates test cases corresponding to each acceptance criterion and any other test scenario it finds relevant. For instance, it writes a test (in the appropriate format, e.g. Python unittest or JavaScript test) to call the new share function and assert that an email was sent and the shared user has access. It could also test error cases (sharing with an invalid email, etc.), demonstrating thorough V&V. The CLI then allows the QA agent to run these tests. Using Claude-Code’s ability to run shell commands, it might execute pytest test_share.py or similar ￼. If running in strict mode, the tool would actually run the code (with user permission or via a configured safe environment) rather than just rely on Claude’s reasoning. This addresses a known limitation: without actual execution, the AI might “simulate” results in its head, which is not always reliable ￼. By integrating real test runs, we enforce ground truth verification.

   - If tests pass: The QA agent reports success. The orchestrator moves to the validation step.
   - If tests fail: The agent (or orchestrator logic) identifies the failures. At this point, AgileVerifFlowCC can do a few things.
     In a fully automated run, it might re-engage the Developer subagent to fix the code immediately based on the failing test output. (Claude, seeing the error trace, can often debug and correct the code.) This essentially creates a rapid fix loop until tests pass – staying within the sprint until the V is closed. In interactive mode, the tool would present the failure to the user and ask whether to let the AI attempt a fix or if the user will fix manually. Either way, the cycle repeats: run tests again until all pass. This loop ensures correctness is enforced before proceeding – a core promise of the V-model that every requirement has matching verification ￼. Hard gating mode would not allow marking the story done until tests are green.

1. Validation (Stakeholder Acceptance): With all tests green, the feature is functionally complete. AgileVerifFlowCC now simulates a validation step. This could be as simple as summarizing the feature’s behavior and checking it against the acceptance criteria in plain language. The Requirements subagent (or perhaps a special “Reviewer” agent) might re-read the original story: “User can share a list via email and recipient can view it” and confirm: “Yes, implemented as: clicking share sends an email with a link, recipient uses link to view – this matches the acceptance criteria.” This provides a final sanity check that we built the right thing, not just built it right. Optionally, the tool might involve the human user here: e.g. open a browser with the running app or show a demo output for the user to approve (if a UI, maybe show a screenshot or log output). In an enterprise setting, this is where a Product Owner would review the increment; in our AI-driven tool, we mimic that via a final QA agent affirmation. We want to ensure nothing was lost in translation – by cross-checking the end result with the initial requirement, we guard against the AI having gone off-track.

1. Integration and Regression Testing: If this story was part of a larger system, the orchestrator can run a quick integration test suite to ensure the new code hasn’t broken existing features. This is essentially re-running all tests (or specifically those that might relate to the changed components). Because the Agile V-Model calls for continuous integration, our tool should facilitate that by possibly invoking a full test run (which could simply be another QA subagent action to run pytest with all tests). In a scenario with multiple teams or components, at certain intervals there might be a dedicated integration sprint where multiple features are tested together ￼. AgileVerifFlowCC can accommodate that by allowing a mode to run broader system tests beyond the scope of one story. For now, assume our single-story sprint includes verifying it integrates well (no test regressions).

1. Artifact Update & Traceability: Before closing the sprint, the tool performs housekeeping to keep long-term artifacts up to date (this step is crucial per Agile V-model principles ￼). The orchestrator updates the backlog.md to mark the story as completed (and possibly record which sprint delivered it). It also might append a brief note in an appendix or a changelog.md: e.g. “Sprint 3: ‘Share To-Do List’ implemented and verified.” The architecture document might be updated with any changes made (if not already done by the design agent). Importantly, any new insights discovered are fed back: for example, if testing revealed a new requirement (“also needed to implement an email opt-out setting”), the tool could either automatically add a new story to the backlog or prompt the user to do so – ensuring nothing “falls through the cracks” ￼. This practice mirrors how real agile teams adjust the backlog and design continuously; our tool formalizes it. The persistent memory (CLAUDE.md) is also updated if needed – maybe adding new commands or gotchas that the AI should remember next time.

1. Closing Sprint N: Finally, the sprint is formally closed. AgileVerifFlowCC could output a Sprint Report that includes: the initial requirement, confirmation of tests passed, any documentation updated, and a summary of time or tokens spent (for user awareness). In interactive usage, the tool might also prompt for a short retrospective note (optionally the user can record lessons learned or the AI can suggest improvements to the process). While not strictly required, this adds a nice agile touch and could feed into the CLAUDE.md as well (e.g. “Note: last sprint had a delay due to lack of clarity in requirements; in future, involve requirements agent more thoroughly.”). After this, the system is ready to start the next sprint with the next story.

Throughout Sprint N, AgileVerifFlowCC enforces the V-model discipline but in an agile wrapper. Each increment delivered is verified and validated, and documentation is continuously updated to reflect the current state ￼ ￼. If the conversation with Claude became long, the orchestrator can at points clear the short-term context (using a /clear or new session) once it has saved all necessary info to files, then reload the context for the next sprint from those files ￼ ￼. This prevents the context window from growing unmanageably large over many sprints while preserving knowledge in long-term memory (the files) – essentially dividing memory into short-term vs long-term, which is a known strategy for scaling LLM-assisted development ￼. The next sprint begins fresh, but with up-to-date docs to pull in relevant context.

Diagram – Sprint Workflow: The following sequence (in text form) summarizes Sprint N workflow:

- Plan: Orchestrator selects story -> Requirements agent refines it (updates backlog).

- Design: Design agent plans implementation (updates architecture/design notes).

- Code: Developer agent writes code (source files updated).

- Verify: QA agent writes & runs tests (green or red). If red, fix code and re-run tests.

- Validate: Confirm output meets acceptance (AI checks criteria or user review).

- Integrate: Run regression tests/integration (ensure no side-effects).

- Document: Update backlog status, architecture, and memory with changes.

- Close: Sprint done; ready for next cycle.

Each sprint thus yields a fully verified increment (a “vertical slice” through the system with corresponding tests), embodying the idea that “each sprint becomes a complete V, including development and integration/test” ￼. This process, enforced by the tool, gives stakeholders higher confidence in each delivery, as every requirement is tied to tests and documentation at all times.
