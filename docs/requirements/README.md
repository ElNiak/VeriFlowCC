# Product Requirements Document (PRD)

**Functional Requirements:** The VerifFlowCC CLI tool must enable **structured, agent-driven software generation** with strict adherence to an Agile V-Model workflow. Key functional requirements include:

\[ \] **Enforced V-Model Stages:** The system **must enforce sequential phases** – Planning, Design, Coding, Testing, and Validation – for each feature or user story. Progression is gated: *e.g.* code generation cannot begin until requirements and design are documented, and testing must succeed before validation completes. Skipping or reordering stages is disallowed without explicit user action (rollback or restart).

\[ \] **Multi-Agent Orchestration:** Utilize a **hierarchical multi-agent approach**.
\- A **Planner agent** (Claude Opus 4.1) orchestrates high-level strategy and planning, the Planner decomposes goals and delegates to sub-agents via a structured workflow.
\- while specialized **Worker sub-agents** (Claude Sonnet 4) execute tasks for each stage (design, coding, testing, review).

\[ \] **Claude Code Integration:** Leverage the Claude Code SDK and commands for agent actions. The agents should be able to **autonomously use Claude Code’s capabilities** – reading/writing files, editing code, running tests/shell commands, managing git – via **slash commands or MCP tools**. For example, the coding agent can apply code edits or run `npm test` through an MCP shell tool, with output fed back into the workflow.

\[ \] **Context Engineering:** Implement robust context management at each stage. The system uses **structured prompt templates** (via Jinja2) to format inputs for each agent role, ensuring they receive *only relevant context*. It must maintain **persistent memory** across sessions (e.g. a `CLAUDE.md` or `MEMORY.md` file logging decisions, requirements, and design choices). Each agent’s prompt context is isolated to its concern – *e.g.* the testing agent sees the new code and test criteria but not the full design discussion.

\[ \] **Plan-Then-Act Workflow:** Enforce a plan-first, then execute paradigm. The Planner agent must produce a clear plan or task checklist covering all V-model steps for the feature. The system will then step through this plan, *one task at a time*, prompting the appropriate sub-agent to act. This ensures the AI “thinks then acts” in a controlled sequence, mirroring best practices like ReACT and SPARC workflows.

\[ \] **Checkpointing and Rollback:** The tool must support **reproducible workflows** with the ability to checkpoint state and roll back. Before any major code changes, the CLI should create a checkpoint (e.g. a git commit or file snapshot). If a stage fails – *e.g.* tests do not pass or a validation check is negative – the system can revert to the last checkpoint and allow the Planner (or developer) to adjust the plan. This guarantees a clean state to retry from and prevents compounding errors.

\[ \] **Human-in-the-Loop Control:** While automation is key, the developer remains in control. The system should allow the user to review and approve critical outputs at each stage. For example, the Planner’s requirements and design output can be confirmed by the user before coding proceeds. Potentially dangerous actions (file writes, executing code) require user confirmation unless a “trusted” or automation mode is enabled. This aligns with Claude Code’s principle of requiring human approval for impactful actions (like running tools or committing code).

# **Non-Functional Requirements:**

- **Modularity & Extensibility:** The architecture must be modular, making it easy to add or modify stages and integrate new tools. Each agent role (planning, coding, testing, etc.) should be implemented as an independent module with clear interfaces, so future workflows (e.g. adding a Security audit stage) can plug in with minimal changes. The design should favor composition of agents/tooling over monolithic prompts.

- **Token Efficiency:** Ensure **minimal token overhead** through context isolation and succinct prompts. Because multiple LLM calls are used, prompts and outputs should be compact and structured. Use **Pydantic schemas** to validate and enforce concise, machine-readable responses (e.g. a JSON plan, a test results report). By giving each agent only the information it needs, the context size per call is reduced, improving cost efficiency. (Opus 4.1 is used sparingly for planning due to its higher cost, while Sonnet 4 handles the bulk of tasks cost-effectively.)

- **Performance:** The pipeline should complete in a reasonable time for a typical feature. Parallelize where possible without breaking the V-model sequence – e.g. running static analysis and security checks concurrently after coding. However, since most V-model stages are sequential by design, latency comes mainly from LLM call time and tool execution time. Opus 4.1’s extended reasoning may take longer per call, so its usage is confined to high-level planning. Caching of intermediate results or prompt embeddings (using Anthropic’s prompt caching up to 1 hour) can be used to speed up iterative runs.

- **Reliability & Robustness:** The system should handle partial failures gracefully. If an agent produces an invalid or incomplete output (e.g. schema validation fails for the Planner’s plan), the orchestrator can detect it and retry or ask the agent for clarification. The workflow should not crash due to an exception; errors are caught and reported to the user with suggestions. Additionally, the design accounts for the possibility of the AI going off-track: observability (below) and human checkpoints help catch and correct such cases early.

- **Observability & Transparency:** Every action the system takes should be observable. Implement **structured logging** for each stage (prompts, model responses, tool outputs) and maintain an **execution trace** that the developer can review. Consider a verbose mode that displays each sub-agent’s prompt and answer, or even a live “dashboard” view of agent actions (as seen in some Claude tool integrations). At the end of a run, provide a **summary report** (e.g. list of tasks done, tests passed, time taken, token usage) similar to Claude Code’s session summary. These features make the agent’s decision process transparent and the results reproducible for auditing.
